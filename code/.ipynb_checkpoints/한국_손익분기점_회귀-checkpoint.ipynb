{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_fin = pd.read_csv('전처리_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi = pd.read_csv('Movie_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin['Target'] = audi['audiAcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_rank</th>\n",
       "      <th>genre_rank</th>\n",
       "      <th>direct_rank</th>\n",
       "      <th>네이버_기대지수</th>\n",
       "      <th>star_buzz</th>\n",
       "      <th>movie_buzz_naver</th>\n",
       "      <th>google_trend</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>1629</td>\n",
       "      <td>22</td>\n",
       "      <td>773.0</td>\n",
       "      <td>723414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>581</td>\n",
       "      <td>19</td>\n",
       "      <td>387.0</td>\n",
       "      <td>351276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>494.0</td>\n",
       "      <td>943</td>\n",
       "      <td>30</td>\n",
       "      <td>616.0</td>\n",
       "      <td>336822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>94</td>\n",
       "      <td>162.0</td>\n",
       "      <td>111</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>800.0</td>\n",
       "      <td>335</td>\n",
       "      <td>5</td>\n",
       "      <td>1674.0</td>\n",
       "      <td>24823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13693.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>219</td>\n",
       "      <td>947.0</td>\n",
       "      <td>7759473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>72</td>\n",
       "      <td>10731.0</td>\n",
       "      <td>1196</td>\n",
       "      <td>124</td>\n",
       "      <td>886.0</td>\n",
       "      <td>955175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4941.0</td>\n",
       "      <td>1413</td>\n",
       "      <td>379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8646758.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>247.0</td>\n",
       "      <td>147</td>\n",
       "      <td>25</td>\n",
       "      <td>459.0</td>\n",
       "      <td>1660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>13877.0</td>\n",
       "      <td>1830</td>\n",
       "      <td>61</td>\n",
       "      <td>2593.0</td>\n",
       "      <td>7231638.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dist_rank  genre_rank  direct_rank  네이버_기대지수  star_buzz  \\\n",
       "0            4           5           67    1274.0       1629   \n",
       "1            8          13           67    3838.0        581   \n",
       "2            2          13           41     494.0        943   \n",
       "3            8          13           94     162.0        111   \n",
       "4            3          13           67     800.0        335   \n",
       "..         ...         ...          ...       ...        ...   \n",
       "570          3          13            5   13693.0       1987   \n",
       "571          2          13           72   10731.0       1196   \n",
       "572          4           1            5    4941.0       1413   \n",
       "573          7           8          100     247.0        147   \n",
       "574          3          13           67   13877.0       1830   \n",
       "\n",
       "     movie_buzz_naver  google_trend     Target  \n",
       "0                  22         773.0   723414.0  \n",
       "1                  19         387.0   351276.0  \n",
       "2                  30         616.0   336822.0  \n",
       "3                   6           0.0     7443.0  \n",
       "4                   5        1674.0    24823.0  \n",
       "..                ...           ...        ...  \n",
       "570               219         947.0  7759473.0  \n",
       "571               124         886.0   955175.0  \n",
       "572               379           0.0  8646758.0  \n",
       "573                25         459.0     1660.0  \n",
       "574                61        2593.0  7231638.0  \n",
       "\n",
       "[575 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_fin['Target']\n",
    "x = df_fin.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화, 정규화하는 것이 더 좋은 결과를 낳음\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    " \n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 선언\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=7, activation='elu'))  # 입력층\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='elu'))      # 은닉층1 \n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='elu'))      # 은닉층1\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='elu'))      # 은닉층1\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(8, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(8, activation='elu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(8, activation='elu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(8, activation='elu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(8, activation='elu'))\n",
    "#model.add(Dense(4, activation='elu'))\n",
    "model.add(Dense(1))# 출력층\n",
    "# 선형 회귀는 마지막에 참과 거짓을 구분할 필요가 없음. 출력층에 활성화 함수를 지정할 필요도 없음\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# 모델 업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_mae', verbose=0, save_best_only=True, mode='min')\n",
    "# 콜백함수\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=100, mode='min')\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_mae', patience=130, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "5/5 [==============================] - 1s 41ms/step - loss: 1360968.7500 - mae: 1360968.7500 - val_loss: 1439345.2500 - val_mae: 1439345.2500 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1360419.8750 - mae: 1360419.8750 - val_loss: 1438757.6250 - val_mae: 1438757.6250 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1359857.5000 - mae: 1359857.5000 - val_loss: 1438064.2500 - val_mae: 1438064.2500 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1359226.5000 - mae: 1359226.5000 - val_loss: 1437039.6250 - val_mae: 1437039.6250 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1358235.6250 - mae: 1358235.6250 - val_loss: 1435755.6250 - val_mae: 1435755.6250 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1357201.6250 - mae: 1357201.6250 - val_loss: 1434332.8750 - val_mae: 1434332.8750 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1355745.0000 - mae: 1355745.0000 - val_loss: 1432395.1250 - val_mae: 1432395.1250 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1353826.3750 - mae: 1353826.3750 - val_loss: 1429923.7500 - val_mae: 1429923.7500 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1350482.8750 - mae: 1350482.8750 - val_loss: 1426856.2500 - val_mae: 1426856.2500 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1347482.0000 - mae: 1347482.0000 - val_loss: 1423281.3750 - val_mae: 1423281.3750 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1344532.2500 - mae: 1344532.2500 - val_loss: 1419126.1250 - val_mae: 1419126.1250 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1339701.1250 - mae: 1339701.1250 - val_loss: 1413987.7500 - val_mae: 1413987.7500 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1334469.6250 - mae: 1334469.6250 - val_loss: 1407514.1250 - val_mae: 1407514.1250 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1328828.6250 - mae: 1328828.6250 - val_loss: 1400162.0000 - val_mae: 1400162.0000 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1322415.0000 - mae: 1322415.0000 - val_loss: 1392724.1250 - val_mae: 1392724.1250 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1319914.0000 - mae: 1319914.0000 - val_loss: 1383855.3750 - val_mae: 1383855.3750 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1307952.7500 - mae: 1307952.7500 - val_loss: 1373898.5000 - val_mae: 1373898.5000 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1298807.3750 - mae: 1298807.3750 - val_loss: 1364043.1250 - val_mae: 1364043.1250 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1275999.6250 - mae: 1275999.6250 - val_loss: 1353122.3750 - val_mae: 1353122.5000 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1288659.5000 - mae: 1288659.5000 - val_loss: 1342592.8750 - val_mae: 1342592.8750 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1262884.8750 - mae: 1262884.8750 - val_loss: 1328270.7500 - val_mae: 1328270.7500 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1258417.2500 - mae: 1258417.2500 - val_loss: 1315085.7500 - val_mae: 1315085.7500 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1237549.7500 - mae: 1237549.7500 - val_loss: 1300547.5000 - val_mae: 1300547.5000 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1244683.1250 - mae: 1244683.1250 - val_loss: 1289468.7500 - val_mae: 1289468.7500 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1207704.0000 - mae: 1207704.0000 - val_loss: 1277644.3750 - val_mae: 1277644.3750 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1201877.3750 - mae: 1201877.3750 - val_loss: 1264722.7500 - val_mae: 1264722.7500 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1212117.2500 - mae: 1212117.2500 - val_loss: 1251324.5000 - val_mae: 1251324.5000 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1185492.6250 - mae: 1185492.6250 - val_loss: 1239048.8750 - val_mae: 1239048.8750 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1198527.8750 - mae: 1198527.8750 - val_loss: 1227817.5000 - val_mae: 1227817.5000 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1158041.7500 - mae: 1158041.7500 - val_loss: 1226293.3750 - val_mae: 1226293.3750 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1197533.7500 - mae: 1197533.7500 - val_loss: 1225133.8750 - val_mae: 1225133.8750 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1135116.1250 - mae: 1135116.1250 - val_loss: 1226326.7500 - val_mae: 1226326.7500 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1128402.5000 - mae: 1128402.5000 - val_loss: 1229709.1250 - val_mae: 1229709.1250 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1122965.5000 - mae: 1122965.5000 - val_loss: 1232023.3750 - val_mae: 1232023.3750 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1124819.6250 - mae: 1124819.6250 - val_loss: 1238775.5000 - val_mae: 1238775.5000 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1113473.8750 - mae: 1113473.8750 - val_loss: 1244733.7500 - val_mae: 1244733.6250 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1133910.0000 - mae: 1133910.0000 - val_loss: 1252844.8750 - val_mae: 1252845.0000 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1068732.7500 - mae: 1068732.7500 - val_loss: 1258075.2500 - val_mae: 1258075.2500 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1095807.3750 - mae: 1095807.3750 - val_loss: 1268738.2500 - val_mae: 1268738.2500 - lr: 0.0010\n",
      "Epoch 40/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1115918.3750 - mae: 1115918.3750 - val_loss: 1272924.1250 - val_mae: 1272924.2500 - lr: 0.0010\n",
      "Epoch 41/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1141400.7500 - mae: 1141400.7500 - val_loss: 1269021.3750 - val_mae: 1269021.3750 - lr: 0.0010\n",
      "Epoch 42/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1125678.2500 - mae: 1125678.2500 - val_loss: 1265254.5000 - val_mae: 1265254.5000 - lr: 0.0010\n",
      "Epoch 43/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1131746.1250 - mae: 1131746.1250 - val_loss: 1278567.2500 - val_mae: 1278567.1250 - lr: 0.0010\n",
      "Epoch 44/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1101861.3750 - mae: 1101861.3750 - val_loss: 1295270.1250 - val_mae: 1295270.1250 - lr: 0.0010\n",
      "Epoch 45/2000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1110012.7500 - mae: 1110012.7500 - val_loss: 1286914.3750 - val_mae: 1286914.3750 - lr: 0.0010\n",
      "Epoch 46/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1138680.8750 - mae: 1138680.8750 - val_loss: 1284617.2500 - val_mae: 1284617.2500 - lr: 0.0010\n",
      "Epoch 47/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1129634.2500 - mae: 1129634.2500 - val_loss: 1285364.6250 - val_mae: 1285364.6250 - lr: 0.0010\n",
      "Epoch 48/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1087662.3750 - mae: 1087662.3750 - val_loss: 1273755.6250 - val_mae: 1273755.6250 - lr: 0.0010\n",
      "Epoch 49/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1114320.8750 - mae: 1114320.8750 - val_loss: 1276683.3750 - val_mae: 1276683.3750 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1123404.5000 - mae: 1123404.5000 - val_loss: 1266042.8750 - val_mae: 1266042.8750 - lr: 0.0010\n",
      "Epoch 51/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1163742.7500 - mae: 1163742.7500 - val_loss: 1261961.8750 - val_mae: 1261961.8750 - lr: 0.0010\n",
      "Epoch 52/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1140266.7500 - mae: 1140266.7500 - val_loss: 1257537.2500 - val_mae: 1257537.2500 - lr: 0.0010\n",
      "Epoch 53/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1079754.0000 - mae: 1079754.0000 - val_loss: 1269221.1250 - val_mae: 1269221.1250 - lr: 0.0010\n",
      "Epoch 54/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1124558.3750 - mae: 1124558.3750 - val_loss: 1272507.1250 - val_mae: 1272507.2500 - lr: 0.0010\n",
      "Epoch 55/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1179292.0000 - mae: 1179292.0000 - val_loss: 1272509.5000 - val_mae: 1272509.3750 - lr: 0.0010\n",
      "Epoch 56/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1119559.7500 - mae: 1119559.7500 - val_loss: 1270881.2500 - val_mae: 1270881.0000 - lr: 0.0010\n",
      "Epoch 57/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1125517.3750 - mae: 1125517.3750 - val_loss: 1264395.6250 - val_mae: 1264395.6250 - lr: 0.0010\n",
      "Epoch 58/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1110387.2500 - mae: 1110387.2500 - val_loss: 1258097.2500 - val_mae: 1258097.2500 - lr: 0.0010\n",
      "Epoch 59/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1143807.6250 - mae: 1143807.6250 - val_loss: 1244907.6250 - val_mae: 1244907.6250 - lr: 0.0010\n",
      "Epoch 60/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1068092.3750 - mae: 1068092.3750 - val_loss: 1253946.7500 - val_mae: 1253946.7500 - lr: 0.0010\n",
      "Epoch 61/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1081037.5000 - mae: 1081037.5000 - val_loss: 1259980.8750 - val_mae: 1259981.0000 - lr: 0.0010\n",
      "Epoch 62/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1101412.0000 - mae: 1101412.0000 - val_loss: 1266032.2500 - val_mae: 1266032.1250 - lr: 0.0010\n",
      "Epoch 63/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1080435.8750 - mae: 1080435.8750 - val_loss: 1271371.8750 - val_mae: 1271371.8750 - lr: 0.0010\n",
      "Epoch 64/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1100191.8750 - mae: 1100191.8750 - val_loss: 1261144.8750 - val_mae: 1261144.8750 - lr: 0.0010\n",
      "Epoch 65/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1095175.0000 - mae: 1095175.0000 - val_loss: 1248182.7500 - val_mae: 1248182.7500 - lr: 0.0010\n",
      "Epoch 66/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1096744.0000 - mae: 1096744.0000 - val_loss: 1253743.3750 - val_mae: 1253743.5000 - lr: 0.0010\n",
      "Epoch 67/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1081423.7500 - mae: 1081423.7500 - val_loss: 1269639.8750 - val_mae: 1269639.8750 - lr: 0.0010\n",
      "Epoch 68/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1083712.8750 - mae: 1083712.8750 - val_loss: 1277646.7500 - val_mae: 1277646.7500 - lr: 0.0010\n",
      "Epoch 69/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1083508.8750 - mae: 1083508.8750 - val_loss: 1284948.5000 - val_mae: 1284948.3750 - lr: 0.0010\n",
      "Epoch 70/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1115699.8750 - mae: 1115699.8750 - val_loss: 1281272.8750 - val_mae: 1281272.8750 - lr: 0.0010\n",
      "Epoch 71/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1109955.0000 - mae: 1109955.0000 - val_loss: 1276132.1250 - val_mae: 1276132.2500 - lr: 0.0010\n",
      "Epoch 72/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1106080.1250 - mae: 1106080.1250 - val_loss: 1268069.8750 - val_mae: 1268069.8750 - lr: 0.0010\n",
      "Epoch 73/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1105085.1250 - mae: 1105085.1250 - val_loss: 1265018.7500 - val_mae: 1265018.7500 - lr: 0.0010\n",
      "Epoch 74/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1096104.6250 - mae: 1096104.6250 - val_loss: 1257428.6250 - val_mae: 1257428.6250 - lr: 0.0010\n",
      "Epoch 75/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1075651.7500 - mae: 1075651.7500 - val_loss: 1263479.5000 - val_mae: 1263479.5000 - lr: 0.0010\n",
      "Epoch 76/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1086035.6250 - mae: 1086035.6250 - val_loss: 1265299.5000 - val_mae: 1265299.5000 - lr: 0.0010\n",
      "Epoch 77/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1089535.5000 - mae: 1089535.5000 - val_loss: 1271833.6250 - val_mae: 1271833.7500 - lr: 0.0010\n",
      "Epoch 78/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1082795.0000 - mae: 1082795.0000 - val_loss: 1259569.7500 - val_mae: 1259569.7500 - lr: 0.0010\n",
      "Epoch 79/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1076928.1250 - mae: 1076928.1250 - val_loss: 1242490.1250 - val_mae: 1242490.3750 - lr: 0.0010\n",
      "Epoch 80/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1083869.1250 - mae: 1083869.1250 - val_loss: 1249373.8750 - val_mae: 1249374.0000 - lr: 0.0010\n",
      "Epoch 81/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1085097.0000 - mae: 1085097.0000 - val_loss: 1237917.6250 - val_mae: 1237917.6250 - lr: 0.0010\n",
      "Epoch 82/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1082771.8750 - mae: 1082771.8750 - val_loss: 1231516.7500 - val_mae: 1231516.5000 - lr: 0.0010\n",
      "Epoch 83/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1072681.8750 - mae: 1072681.8750 - val_loss: 1231153.5000 - val_mae: 1231153.5000 - lr: 0.0010\n",
      "Epoch 84/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1071090.8750 - mae: 1071090.8750 - val_loss: 1228516.7500 - val_mae: 1228516.7500 - lr: 0.0010\n",
      "Epoch 85/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1064632.6250 - mae: 1064632.6250 - val_loss: 1226083.5000 - val_mae: 1226083.5000 - lr: 0.0010\n",
      "Epoch 86/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1115095.1250 - mae: 1115095.1250 - val_loss: 1226118.2500 - val_mae: 1226118.2500 - lr: 0.0010\n",
      "Epoch 87/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1063347.5000 - mae: 1063347.5000 - val_loss: 1229343.7500 - val_mae: 1229343.7500 - lr: 0.0010\n",
      "Epoch 88/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1060192.3750 - mae: 1060192.3750 - val_loss: 1236526.2500 - val_mae: 1236526.2500 - lr: 0.0010\n",
      "Epoch 89/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1084179.7500 - mae: 1084179.7500 - val_loss: 1235489.8750 - val_mae: 1235489.8750 - lr: 0.0010\n",
      "Epoch 90/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1064631.7500 - mae: 1064631.7500 - val_loss: 1248585.0000 - val_mae: 1248585.0000 - lr: 0.0010\n",
      "Epoch 91/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1080705.6250 - mae: 1080705.6250 - val_loss: 1242603.5000 - val_mae: 1242603.5000 - lr: 0.0010\n",
      "Epoch 92/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1089094.0000 - mae: 1089094.0000 - val_loss: 1236869.8750 - val_mae: 1236869.8750 - lr: 0.0010\n",
      "Epoch 93/2000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1061776.6250 - mae: 1061776.6250 - val_loss: 1218445.0000 - val_mae: 1218445.0000 - lr: 0.0010\n",
      "Epoch 94/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1033376.3125 - mae: 1033376.3125 - val_loss: 1229382.2500 - val_mae: 1229382.2500 - lr: 0.0010\n",
      "Epoch 95/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1066820.8750 - mae: 1066820.8750 - val_loss: 1238587.0000 - val_mae: 1238587.0000 - lr: 0.0010\n",
      "Epoch 96/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1051190.8750 - mae: 1051190.8750 - val_loss: 1219348.3750 - val_mae: 1219348.3750 - lr: 0.0010\n",
      "Epoch 97/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1014274.3750 - mae: 1014274.3750 - val_loss: 1216772.8750 - val_mae: 1216772.8750 - lr: 0.0010\n",
      "Epoch 98/2000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1093486.8750 - mae: 1093486.8750 - val_loss: 1208307.5000 - val_mae: 1208307.5000 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1071580.0000 - mae: 1071580.0000 - val_loss: 1212874.0000 - val_mae: 1212874.0000 - lr: 0.0010\n",
      "Epoch 100/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1064972.0000 - mae: 1064972.0000 - val_loss: 1223929.3750 - val_mae: 1223929.3750 - lr: 0.0010\n",
      "Epoch 101/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1087812.5000 - mae: 1087812.5000 - val_loss: 1205254.6250 - val_mae: 1205254.6250 - lr: 0.0010\n",
      "Epoch 102/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1058159.3750 - mae: 1058159.3750 - val_loss: 1205670.0000 - val_mae: 1205670.0000 - lr: 0.0010\n",
      "Epoch 103/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1022358.0000 - mae: 1022358.0000 - val_loss: 1222038.8750 - val_mae: 1222038.8750 - lr: 0.0010\n",
      "Epoch 104/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1070638.2500 - mae: 1070638.2500 - val_loss: 1207440.1250 - val_mae: 1207440.1250 - lr: 0.0010\n",
      "Epoch 105/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1048828.6250 - mae: 1048828.6250 - val_loss: 1211791.6250 - val_mae: 1211791.6250 - lr: 0.0010\n",
      "Epoch 106/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1035727.5000 - mae: 1035727.5000 - val_loss: 1207590.1250 - val_mae: 1207590.1250 - lr: 0.0010\n",
      "Epoch 107/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1052182.6250 - mae: 1052182.6250 - val_loss: 1206580.6250 - val_mae: 1206580.6250 - lr: 0.0010\n",
      "Epoch 108/2000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1063034.2500 - mae: 1063034.2500 - val_loss: 1198448.0000 - val_mae: 1198448.0000 - lr: 0.0010\n",
      "Epoch 109/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1063076.3750 - mae: 1063076.3750 - val_loss: 1207281.7500 - val_mae: 1207281.7500 - lr: 0.0010\n",
      "Epoch 110/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1109603.2500 - mae: 1109603.2500 - val_loss: 1206372.5000 - val_mae: 1206372.5000 - lr: 0.0010\n",
      "Epoch 111/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1039950.2500 - mae: 1039950.2500 - val_loss: 1219352.7500 - val_mae: 1219352.7500 - lr: 0.0010\n",
      "Epoch 112/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1081432.2500 - mae: 1081432.2500 - val_loss: 1211041.0000 - val_mae: 1211041.0000 - lr: 0.0010\n",
      "Epoch 113/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1052709.6250 - mae: 1052709.6250 - val_loss: 1193895.1250 - val_mae: 1193895.1250 - lr: 0.0010\n",
      "Epoch 114/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1059974.8750 - mae: 1059974.8750 - val_loss: 1172942.2500 - val_mae: 1172942.2500 - lr: 0.0010\n",
      "Epoch 115/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1066051.2500 - mae: 1066051.2500 - val_loss: 1170594.1250 - val_mae: 1170594.1250 - lr: 0.0010\n",
      "Epoch 116/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1068104.3750 - mae: 1068104.3750 - val_loss: 1173020.8750 - val_mae: 1173020.8750 - lr: 0.0010\n",
      "Epoch 117/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1046578.0000 - mae: 1046578.0000 - val_loss: 1174235.8750 - val_mae: 1174235.8750 - lr: 0.0010\n",
      "Epoch 118/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1052038.6250 - mae: 1052038.6250 - val_loss: 1161750.3750 - val_mae: 1161750.3750 - lr: 0.0010\n",
      "Epoch 119/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1037413.8125 - mae: 1037413.8125 - val_loss: 1163872.7500 - val_mae: 1163872.7500 - lr: 0.0010\n",
      "Epoch 120/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1030317.1250 - mae: 1030317.1250 - val_loss: 1176165.6250 - val_mae: 1176165.6250 - lr: 0.0010\n",
      "Epoch 121/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1046939.5625 - mae: 1046939.5625 - val_loss: 1179579.2500 - val_mae: 1179579.2500 - lr: 0.0010\n",
      "Epoch 122/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1085006.0000 - mae: 1085006.0000 - val_loss: 1157948.2500 - val_mae: 1157948.2500 - lr: 0.0010\n",
      "Epoch 123/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1048405.5000 - mae: 1048405.5000 - val_loss: 1169686.6250 - val_mae: 1169686.6250 - lr: 0.0010\n",
      "Epoch 124/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1031135.1875 - mae: 1031135.1875 - val_loss: 1154822.2500 - val_mae: 1154822.2500 - lr: 0.0010\n",
      "Epoch 125/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1030113.0000 - mae: 1030113.0000 - val_loss: 1159325.8750 - val_mae: 1159325.8750 - lr: 0.0010\n",
      "Epoch 126/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1028127.4375 - mae: 1028127.4375 - val_loss: 1164183.7500 - val_mae: 1164183.7500 - lr: 0.0010\n",
      "Epoch 127/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 988784.1875 - mae: 988784.1875 - val_loss: 1174270.3750 - val_mae: 1174270.3750 - lr: 0.0010\n",
      "Epoch 128/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1003406.6875 - mae: 1003406.6875 - val_loss: 1189312.8750 - val_mae: 1189312.8750 - lr: 0.0010\n",
      "Epoch 129/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1066298.0000 - mae: 1066298.0000 - val_loss: 1183295.0000 - val_mae: 1183295.0000 - lr: 0.0010\n",
      "Epoch 130/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1052309.6250 - mae: 1052309.6250 - val_loss: 1181257.0000 - val_mae: 1181257.0000 - lr: 0.0010\n",
      "Epoch 131/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1049531.1250 - mae: 1049531.1250 - val_loss: 1156912.5000 - val_mae: 1156912.5000 - lr: 0.0010\n",
      "Epoch 132/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1040741.3750 - mae: 1040741.3750 - val_loss: 1147407.5000 - val_mae: 1147407.5000 - lr: 0.0010\n",
      "Epoch 133/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1039231.4375 - mae: 1039231.4375 - val_loss: 1132507.6250 - val_mae: 1132507.6250 - lr: 0.0010\n",
      "Epoch 134/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1058118.5000 - mae: 1058118.5000 - val_loss: 1133934.7500 - val_mae: 1133934.7500 - lr: 0.0010\n",
      "Epoch 135/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1048203.7500 - mae: 1048203.7500 - val_loss: 1146153.1250 - val_mae: 1146153.1250 - lr: 0.0010\n",
      "Epoch 136/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1020445.8125 - mae: 1020445.8125 - val_loss: 1139796.8750 - val_mae: 1139796.8750 - lr: 0.0010\n",
      "Epoch 137/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 991529.4375 - mae: 991529.4375 - val_loss: 1146978.0000 - val_mae: 1146978.0000 - lr: 0.0010\n",
      "Epoch 138/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1027971.5000 - mae: 1027971.5000 - val_loss: 1134330.3750 - val_mae: 1134330.3750 - lr: 0.0010\n",
      "Epoch 139/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 971766.3125 - mae: 971766.3125 - val_loss: 1136449.3750 - val_mae: 1136449.3750 - lr: 0.0010\n",
      "Epoch 140/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1029334.8750 - mae: 1029334.8750 - val_loss: 1138236.0000 - val_mae: 1138236.0000 - lr: 0.0010\n",
      "Epoch 141/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1011995.4375 - mae: 1011995.4375 - val_loss: 1143558.7500 - val_mae: 1143558.7500 - lr: 0.0010\n",
      "Epoch 142/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1042891.8125 - mae: 1042891.8125 - val_loss: 1131267.8750 - val_mae: 1131267.8750 - lr: 0.0010\n",
      "Epoch 143/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1006930.8125 - mae: 1006930.8125 - val_loss: 1135013.2500 - val_mae: 1135013.2500 - lr: 0.0010\n",
      "Epoch 144/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1032838.6250 - mae: 1032838.6250 - val_loss: 1138900.2500 - val_mae: 1138900.2500 - lr: 0.0010\n",
      "Epoch 145/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1017348.6875 - mae: 1017348.6875 - val_loss: 1146965.8750 - val_mae: 1146965.8750 - lr: 0.0010\n",
      "Epoch 146/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1014480.3125 - mae: 1014480.3125 - val_loss: 1143813.0000 - val_mae: 1143813.0000 - lr: 0.0010\n",
      "Epoch 147/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 8ms/step - loss: 1015542.1875 - mae: 1015542.1875 - val_loss: 1138902.1250 - val_mae: 1138902.1250 - lr: 0.0010\n",
      "Epoch 148/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1054246.0000 - mae: 1054246.0000 - val_loss: 1119830.7500 - val_mae: 1119830.7500 - lr: 0.0010\n",
      "Epoch 149/2000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1052737.1250 - mae: 1052737.1250 - val_loss: 1114448.8750 - val_mae: 1114448.8750 - lr: 0.0010\n",
      "Epoch 150/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 988965.0000 - mae: 988965.0000 - val_loss: 1125652.3750 - val_mae: 1125652.3750 - lr: 0.0010\n",
      "Epoch 151/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1043426.5000 - mae: 1043426.5000 - val_loss: 1121063.6250 - val_mae: 1121063.6250 - lr: 0.0010\n",
      "Epoch 152/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 975554.5000 - mae: 975554.5000 - val_loss: 1129193.6250 - val_mae: 1129193.6250 - lr: 0.0010\n",
      "Epoch 153/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1025550.1250 - mae: 1025550.1250 - val_loss: 1123193.6250 - val_mae: 1123193.6250 - lr: 0.0010\n",
      "Epoch 154/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1017061.0000 - mae: 1017061.0000 - val_loss: 1120219.6250 - val_mae: 1120219.6250 - lr: 0.0010\n",
      "Epoch 155/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1016019.9375 - mae: 1016019.9375 - val_loss: 1120994.8750 - val_mae: 1120994.8750 - lr: 0.0010\n",
      "Epoch 156/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 966375.0625 - mae: 966375.0625 - val_loss: 1123465.7500 - val_mae: 1123465.7500 - lr: 0.0010\n",
      "Epoch 157/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1003567.9375 - mae: 1003567.9375 - val_loss: 1130767.0000 - val_mae: 1130767.0000 - lr: 0.0010\n",
      "Epoch 158/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1008754.3125 - mae: 1008754.3125 - val_loss: 1116219.5000 - val_mae: 1116219.5000 - lr: 0.0010\n",
      "Epoch 159/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1008834.8750 - mae: 1008834.8750 - val_loss: 1111856.3750 - val_mae: 1111856.3750 - lr: 0.0010\n",
      "Epoch 160/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1021310.6250 - mae: 1021310.6250 - val_loss: 1102107.0000 - val_mae: 1102107.0000 - lr: 0.0010\n",
      "Epoch 161/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 993277.8125 - mae: 993277.8125 - val_loss: 1103011.7500 - val_mae: 1103011.7500 - lr: 0.0010\n",
      "Epoch 162/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 999348.2500 - mae: 999348.2500 - val_loss: 1106209.8750 - val_mae: 1106209.8750 - lr: 0.0010\n",
      "Epoch 163/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1035972.0000 - mae: 1035972.0000 - val_loss: 1105814.1250 - val_mae: 1105814.1250 - lr: 0.0010\n",
      "Epoch 164/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1008893.5000 - mae: 1008893.5000 - val_loss: 1106846.6250 - val_mae: 1106846.6250 - lr: 0.0010\n",
      "Epoch 165/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1015987.9375 - mae: 1015987.9375 - val_loss: 1103473.2500 - val_mae: 1103473.2500 - lr: 0.0010\n",
      "Epoch 166/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 973139.2500 - mae: 973139.2500 - val_loss: 1104722.1250 - val_mae: 1104722.1250 - lr: 0.0010\n",
      "Epoch 167/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1028971.7500 - mae: 1028971.7500 - val_loss: 1091416.0000 - val_mae: 1091416.0000 - lr: 0.0010\n",
      "Epoch 168/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1032324.1875 - mae: 1032324.1875 - val_loss: 1082482.3750 - val_mae: 1082482.3750 - lr: 0.0010\n",
      "Epoch 169/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1018971.8125 - mae: 1018971.8125 - val_loss: 1070475.6250 - val_mae: 1070475.6250 - lr: 0.0010\n",
      "Epoch 170/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1047254.5625 - mae: 1047254.5625 - val_loss: 1063834.6250 - val_mae: 1063834.6250 - lr: 0.0010\n",
      "Epoch 171/2000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 958914.9375 - mae: 958914.9375 - val_loss: 1072845.6250 - val_mae: 1072845.6250 - lr: 0.0010\n",
      "Epoch 172/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1014497.6250 - mae: 1014497.6250 - val_loss: 1066740.1250 - val_mae: 1066740.1250 - lr: 0.0010\n",
      "Epoch 173/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 975668.8125 - mae: 975668.8125 - val_loss: 1072160.0000 - val_mae: 1072160.0000 - lr: 0.0010\n",
      "Epoch 174/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 979097.7500 - mae: 979097.7500 - val_loss: 1080752.7500 - val_mae: 1080752.7500 - lr: 0.0010\n",
      "Epoch 175/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1005686.4375 - mae: 1005686.4375 - val_loss: 1076514.7500 - val_mae: 1076514.7500 - lr: 0.0010\n",
      "Epoch 176/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1014498.6250 - mae: 1014498.6250 - val_loss: 1080416.1250 - val_mae: 1080416.1250 - lr: 0.0010\n",
      "Epoch 177/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1001758.8750 - mae: 1001758.8750 - val_loss: 1074445.0000 - val_mae: 1074445.0000 - lr: 0.0010\n",
      "Epoch 178/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1022155.3750 - mae: 1022155.3750 - val_loss: 1081575.3750 - val_mae: 1081575.3750 - lr: 0.0010\n",
      "Epoch 179/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 960695.5625 - mae: 960695.5625 - val_loss: 1088120.1250 - val_mae: 1088120.1250 - lr: 0.0010\n",
      "Epoch 180/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 947291.8750 - mae: 947291.8750 - val_loss: 1094868.5000 - val_mae: 1094868.5000 - lr: 0.0010\n",
      "Epoch 181/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1015311.8125 - mae: 1015311.8125 - val_loss: 1090321.7500 - val_mae: 1090321.7500 - lr: 0.0010\n",
      "Epoch 182/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1020612.0000 - mae: 1020612.0000 - val_loss: 1088475.2500 - val_mae: 1088475.2500 - lr: 0.0010\n",
      "Epoch 183/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1039385.6250 - mae: 1039385.6250 - val_loss: 1091115.6250 - val_mae: 1091115.6250 - lr: 0.0010\n",
      "Epoch 184/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1001239.3750 - mae: 1001239.3750 - val_loss: 1090719.2500 - val_mae: 1090719.2500 - lr: 0.0010\n",
      "Epoch 185/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1048049.8750 - mae: 1048049.8750 - val_loss: 1078187.5000 - val_mae: 1078187.5000 - lr: 0.0010\n",
      "Epoch 186/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 959843.5000 - mae: 959843.5000 - val_loss: 1076972.3750 - val_mae: 1076972.3750 - lr: 0.0010\n",
      "Epoch 187/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 970355.6875 - mae: 970355.6875 - val_loss: 1089053.5000 - val_mae: 1089053.5000 - lr: 0.0010\n",
      "Epoch 188/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 970087.3125 - mae: 970087.3125 - val_loss: 1084507.2500 - val_mae: 1084507.2500 - lr: 0.0010\n",
      "Epoch 189/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 994951.5000 - mae: 994951.5000 - val_loss: 1078174.7500 - val_mae: 1078174.7500 - lr: 0.0010\n",
      "Epoch 190/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 992551.7500 - mae: 992551.7500 - val_loss: 1075089.2500 - val_mae: 1075089.2500 - lr: 0.0010\n",
      "Epoch 191/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 986218.1250 - mae: 986218.1250 - val_loss: 1067386.3750 - val_mae: 1067386.3750 - lr: 0.0010\n",
      "Epoch 192/2000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1012381.9375 - mae: 1012381.9375 - val_loss: 1050747.8750 - val_mae: 1050747.8750 - lr: 0.0010\n",
      "Epoch 193/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 980530.2500 - mae: 980530.2500 - val_loss: 1051404.8750 - val_mae: 1051404.8750 - lr: 0.0010\n",
      "Epoch 194/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1001773.1250 - mae: 1001773.1250 - val_loss: 1049070.5000 - val_mae: 1049070.5000 - lr: 0.0010\n",
      "Epoch 195/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 988328.6250 - mae: 988328.6250 - val_loss: 1044237.1875 - val_mae: 1044237.1875 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 974570.8125 - mae: 974570.8125 - val_loss: 1048811.7500 - val_mae: 1048811.7500 - lr: 0.0010\n",
      "Epoch 197/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1014166.7500 - mae: 1014166.7500 - val_loss: 1036470.9375 - val_mae: 1036470.9375 - lr: 0.0010\n",
      "Epoch 198/2000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 989645.0000 - mae: 989645.0000 - val_loss: 1038684.5000 - val_mae: 1038684.5000 - lr: 0.0010\n",
      "Epoch 199/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 954061.5625 - mae: 954061.5625 - val_loss: 1041194.1875 - val_mae: 1041194.1875 - lr: 0.0010\n",
      "Epoch 200/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 975758.5625 - mae: 975758.5625 - val_loss: 1045664.1250 - val_mae: 1045664.1250 - lr: 0.0010\n",
      "Epoch 201/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1010038.7500 - mae: 1010038.7500 - val_loss: 1039400.4375 - val_mae: 1039400.4375 - lr: 0.0010\n",
      "Epoch 202/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1005622.6875 - mae: 1005622.6875 - val_loss: 1024725.5625 - val_mae: 1024725.5625 - lr: 0.0010\n",
      "Epoch 203/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 961599.1875 - mae: 961599.1875 - val_loss: 1024880.9375 - val_mae: 1024880.9375 - lr: 0.0010\n",
      "Epoch 204/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 957634.0625 - mae: 957634.0625 - val_loss: 1034880.9375 - val_mae: 1034880.9375 - lr: 0.0010\n",
      "Epoch 205/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 988644.6875 - mae: 988644.6875 - val_loss: 1023382.7500 - val_mae: 1023382.7500 - lr: 0.0010\n",
      "Epoch 206/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 969729.1250 - mae: 969729.1250 - val_loss: 1030038.5000 - val_mae: 1030038.5000 - lr: 0.0010\n",
      "Epoch 207/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1001205.6250 - mae: 1001205.6250 - val_loss: 1032976.5000 - val_mae: 1032976.5000 - lr: 0.0010\n",
      "Epoch 208/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1008393.3125 - mae: 1008393.3125 - val_loss: 1033870.2500 - val_mae: 1033870.2500 - lr: 0.0010\n",
      "Epoch 209/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 995932.6875 - mae: 995932.6875 - val_loss: 1031013.8125 - val_mae: 1031013.8125 - lr: 0.0010\n",
      "Epoch 210/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 997298.2500 - mae: 997298.2500 - val_loss: 1032262.8125 - val_mae: 1032262.8125 - lr: 0.0010\n",
      "Epoch 211/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 995647.1875 - mae: 995647.1875 - val_loss: 1033321.9375 - val_mae: 1033321.9375 - lr: 0.0010\n",
      "Epoch 212/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 934951.3750 - mae: 934951.3750 - val_loss: 1027945.8125 - val_mae: 1027945.8125 - lr: 0.0010\n",
      "Epoch 213/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1016891.8750 - mae: 1016891.8750 - val_loss: 1011986.3750 - val_mae: 1011986.3750 - lr: 0.0010\n",
      "Epoch 214/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 921572.5000 - mae: 921572.5000 - val_loss: 1010988.2500 - val_mae: 1010988.2500 - lr: 0.0010\n",
      "Epoch 215/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 963002.0625 - mae: 963002.0625 - val_loss: 1017647.5000 - val_mae: 1017647.5000 - lr: 0.0010\n",
      "Epoch 216/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 962753.3750 - mae: 962753.3750 - val_loss: 1013383.5625 - val_mae: 1013383.5625 - lr: 0.0010\n",
      "Epoch 217/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 928403.5000 - mae: 928403.5000 - val_loss: 1012158.8750 - val_mae: 1012158.8750 - lr: 0.0010\n",
      "Epoch 218/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 961519.5000 - mae: 961519.5000 - val_loss: 1022443.1250 - val_mae: 1022443.1250 - lr: 0.0010\n",
      "Epoch 219/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 958268.5000 - mae: 958268.5000 - val_loss: 1025540.5000 - val_mae: 1025540.5000 - lr: 0.0010\n",
      "Epoch 220/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 992097.1250 - mae: 992097.1250 - val_loss: 1015227.1250 - val_mae: 1015227.1250 - lr: 0.0010\n",
      "Epoch 221/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 966375.7500 - mae: 966375.7500 - val_loss: 1009840.9375 - val_mae: 1009840.9375 - lr: 0.0010\n",
      "Epoch 222/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 994781.1250 - mae: 994781.1250 - val_loss: 1009726.5625 - val_mae: 1009726.5625 - lr: 0.0010\n",
      "Epoch 223/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 922406.9375 - mae: 922406.9375 - val_loss: 1010374.1875 - val_mae: 1010374.1875 - lr: 0.0010\n",
      "Epoch 224/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 931624.0000 - mae: 931624.0000 - val_loss: 1004171.3750 - val_mae: 1004171.3750 - lr: 0.0010\n",
      "Epoch 225/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 906126.6875 - mae: 906126.6875 - val_loss: 1008818.6875 - val_mae: 1008818.6875 - lr: 0.0010\n",
      "Epoch 226/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1001579.3750 - mae: 1001579.3750 - val_loss: 1002998.3750 - val_mae: 1002998.3750 - lr: 0.0010\n",
      "Epoch 227/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 928084.6250 - mae: 928084.6250 - val_loss: 1010068.3750 - val_mae: 1010068.3750 - lr: 0.0010\n",
      "Epoch 228/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 962713.1875 - mae: 962713.1875 - val_loss: 1021196.1875 - val_mae: 1021196.1875 - lr: 0.0010\n",
      "Epoch 229/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1010036.0625 - mae: 1010036.0625 - val_loss: 1015275.0000 - val_mae: 1015275.0000 - lr: 0.0010\n",
      "Epoch 230/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 990332.5625 - mae: 990332.5625 - val_loss: 1014561.1875 - val_mae: 1014561.1875 - lr: 0.0010\n",
      "Epoch 231/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 996871.3750 - mae: 996871.3750 - val_loss: 1006473.8125 - val_mae: 1006473.8125 - lr: 0.0010\n",
      "Epoch 232/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 919917.9375 - mae: 919917.9375 - val_loss: 1017163.8750 - val_mae: 1017163.8750 - lr: 0.0010\n",
      "Epoch 233/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 994835.8125 - mae: 994835.8125 - val_loss: 1012904.8750 - val_mae: 1012904.8750 - lr: 0.0010\n",
      "Epoch 234/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 957858.7500 - mae: 957858.7500 - val_loss: 1003349.9375 - val_mae: 1003349.9375 - lr: 0.0010\n",
      "Epoch 235/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 976427.1875 - mae: 976427.1875 - val_loss: 998037.3750 - val_mae: 998037.3750 - lr: 0.0010\n",
      "Epoch 236/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 931830.5000 - mae: 931830.5000 - val_loss: 1000385.3125 - val_mae: 1000385.3125 - lr: 0.0010\n",
      "Epoch 237/2000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 960216.8125 - mae: 960216.8125 - val_loss: 1004965.8125 - val_mae: 1004965.8125 - lr: 0.0010\n",
      "Epoch 238/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 978482.5625 - mae: 978482.5625 - val_loss: 1002658.8125 - val_mae: 1002658.8125 - lr: 0.0010\n",
      "Epoch 239/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 939223.5625 - mae: 939223.5625 - val_loss: 1011945.1250 - val_mae: 1011945.1250 - lr: 0.0010\n",
      "Epoch 240/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 950769.3125 - mae: 950769.3125 - val_loss: 1009263.3125 - val_mae: 1009263.3125 - lr: 0.0010\n",
      "Epoch 241/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1048168.4375 - mae: 1048168.4375 - val_loss: 1008499.1875 - val_mae: 1008499.1875 - lr: 0.0010\n",
      "Epoch 242/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 949666.0625 - mae: 949666.0625 - val_loss: 1003039.9375 - val_mae: 1003039.9375 - lr: 0.0010\n",
      "Epoch 243/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 981253.4375 - mae: 981253.4375 - val_loss: 1003293.7500 - val_mae: 1003293.7500 - lr: 0.0010\n",
      "Epoch 244/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 975553.6875 - mae: 975553.6875 - val_loss: 999397.5625 - val_mae: 999397.5625 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 956234.3750 - mae: 956234.3750 - val_loss: 1004499.3125 - val_mae: 1004499.3125 - lr: 0.0010\n",
      "Epoch 246/2000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 905385.1250 - mae: 905385.1250 - val_loss: 991702.6250 - val_mae: 991702.6250 - lr: 0.0010\n",
      "Epoch 247/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 936931.6875 - mae: 936931.6875 - val_loss: 992666.5000 - val_mae: 992666.5000 - lr: 0.0010\n",
      "Epoch 248/2000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1009902.0000 - mae: 1009902.0000 - val_loss: 992175.2500 - val_mae: 992175.2500 - lr: 0.0010\n",
      "Epoch 249/2000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 954409.8125 - mae: 954409.8125 - val_loss: 988672.9375 - val_mae: 988672.9375 - lr: 0.0010\n",
      "Epoch 250/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1025370.3125 - mae: 1025370.3125 - val_loss: 990718.1250 - val_mae: 990718.1250 - lr: 0.0010\n",
      "Epoch 251/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 978298.3125 - mae: 978298.3125 - val_loss: 993184.5000 - val_mae: 993184.5000 - lr: 0.0010\n",
      "Epoch 252/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 992770.0625 - mae: 992770.0625 - val_loss: 992900.1875 - val_mae: 992900.1875 - lr: 0.0010\n",
      "Epoch 253/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 928685.6875 - mae: 928685.6875 - val_loss: 997196.8750 - val_mae: 997196.8750 - lr: 0.0010\n",
      "Epoch 254/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 940211.5000 - mae: 940211.5000 - val_loss: 998110.1250 - val_mae: 998110.1250 - lr: 0.0010\n",
      "Epoch 255/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 966936.0000 - mae: 966936.0000 - val_loss: 989171.4375 - val_mae: 989171.4375 - lr: 0.0010\n",
      "Epoch 256/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 983428.3125 - mae: 983428.3125 - val_loss: 982002.7500 - val_mae: 982002.7500 - lr: 0.0010\n",
      "Epoch 257/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 964013.5625 - mae: 964013.5625 - val_loss: 981168.8125 - val_mae: 981168.8125 - lr: 0.0010\n",
      "Epoch 258/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 979685.8125 - mae: 979685.8125 - val_loss: 984643.9375 - val_mae: 984643.9375 - lr: 0.0010\n",
      "Epoch 259/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 988759.8125 - mae: 988759.8125 - val_loss: 983309.0625 - val_mae: 983309.0625 - lr: 0.0010\n",
      "Epoch 260/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1023567.0625 - mae: 1023567.0625 - val_loss: 979671.1250 - val_mae: 979671.1250 - lr: 0.0010\n",
      "Epoch 261/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 956100.4375 - mae: 956100.4375 - val_loss: 984050.0000 - val_mae: 984050.0000 - lr: 0.0010\n",
      "Epoch 262/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1005982.7500 - mae: 1005982.7500 - val_loss: 979106.5000 - val_mae: 979106.5000 - lr: 0.0010\n",
      "Epoch 263/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1010707.3750 - mae: 1010707.3750 - val_loss: 985879.6875 - val_mae: 985879.6875 - lr: 0.0010\n",
      "Epoch 264/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 998795.7500 - mae: 998795.7500 - val_loss: 981076.0625 - val_mae: 981076.0625 - lr: 0.0010\n",
      "Epoch 265/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 918598.9375 - mae: 918598.9375 - val_loss: 981318.3750 - val_mae: 981318.3750 - lr: 0.0010\n",
      "Epoch 266/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 948690.4375 - mae: 948690.4375 - val_loss: 983598.9375 - val_mae: 983598.9375 - lr: 0.0010\n",
      "Epoch 267/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 949573.4375 - mae: 949573.4375 - val_loss: 981992.7500 - val_mae: 981992.7500 - lr: 0.0010\n",
      "Epoch 268/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 913223.6250 - mae: 913223.6250 - val_loss: 975238.4375 - val_mae: 975238.4375 - lr: 0.0010\n",
      "Epoch 269/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 971775.8750 - mae: 971775.8750 - val_loss: 972598.3750 - val_mae: 972598.3750 - lr: 0.0010\n",
      "Epoch 270/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 961187.1875 - mae: 961187.1875 - val_loss: 972501.8125 - val_mae: 972501.8125 - lr: 0.0010\n",
      "Epoch 271/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 923427.5000 - mae: 923427.5000 - val_loss: 973264.6875 - val_mae: 973264.6875 - lr: 0.0010\n",
      "Epoch 272/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 941027.4375 - mae: 941027.4375 - val_loss: 974193.0625 - val_mae: 974193.0625 - lr: 0.0010\n",
      "Epoch 273/2000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 957569.1250 - mae: 957569.1250 - val_loss: 968987.8750 - val_mae: 968987.8750 - lr: 0.0010\n",
      "Epoch 274/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 962186.5625 - mae: 962186.5625 - val_loss: 974022.8750 - val_mae: 974022.8750 - lr: 0.0010\n",
      "Epoch 275/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 959104.0000 - mae: 959104.0000 - val_loss: 982083.4375 - val_mae: 982083.4375 - lr: 0.0010\n",
      "Epoch 276/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 970921.2500 - mae: 970921.2500 - val_loss: 978791.3750 - val_mae: 978791.3750 - lr: 0.0010\n",
      "Epoch 277/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 949810.0625 - mae: 949810.0625 - val_loss: 977329.5625 - val_mae: 977329.5625 - lr: 0.0010\n",
      "Epoch 278/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 952419.0625 - mae: 952419.0625 - val_loss: 977154.6250 - val_mae: 977154.6250 - lr: 0.0010\n",
      "Epoch 279/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 913857.3750 - mae: 913857.3750 - val_loss: 976697.1250 - val_mae: 976697.1250 - lr: 0.0010\n",
      "Epoch 280/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 945501.9375 - mae: 945501.9375 - val_loss: 984726.8750 - val_mae: 984726.8750 - lr: 0.0010\n",
      "Epoch 281/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 964138.0000 - mae: 964138.0000 - val_loss: 982176.9375 - val_mae: 982176.9375 - lr: 0.0010\n",
      "Epoch 282/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 987376.7500 - mae: 987376.7500 - val_loss: 978019.1875 - val_mae: 978019.1875 - lr: 0.0010\n",
      "Epoch 283/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1013754.3125 - mae: 1013754.3125 - val_loss: 974195.3125 - val_mae: 974195.3125 - lr: 0.0010\n",
      "Epoch 284/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 903161.3750 - mae: 903161.3750 - val_loss: 982047.0625 - val_mae: 982047.0625 - lr: 0.0010\n",
      "Epoch 285/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 920858.8125 - mae: 920858.8125 - val_loss: 989622.8750 - val_mae: 989622.8750 - lr: 0.0010\n",
      "Epoch 286/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 967806.8750 - mae: 967806.8750 - val_loss: 991086.6875 - val_mae: 991086.6875 - lr: 0.0010\n",
      "Epoch 287/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 979528.0000 - mae: 979528.0000 - val_loss: 979202.7500 - val_mae: 979202.7500 - lr: 0.0010\n",
      "Epoch 288/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 943386.3125 - mae: 943386.3125 - val_loss: 969138.1875 - val_mae: 969138.1875 - lr: 0.0010\n",
      "Epoch 289/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 968327.7500 - mae: 968327.7500 - val_loss: 967619.0000 - val_mae: 967619.0000 - lr: 0.0010\n",
      "Epoch 290/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 942680.6875 - mae: 942680.6875 - val_loss: 972044.8750 - val_mae: 972044.8750 - lr: 0.0010\n",
      "Epoch 291/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 997954.0625 - mae: 997954.0625 - val_loss: 972102.0000 - val_mae: 972102.0000 - lr: 0.0010\n",
      "Epoch 292/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1000528.6250 - mae: 1000528.6250 - val_loss: 976789.9375 - val_mae: 976789.9375 - lr: 0.0010\n",
      "Epoch 293/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 934076.8125 - mae: 934076.8125 - val_loss: 984570.4375 - val_mae: 984570.4375 - lr: 0.0010\n",
      "Epoch 294/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 8ms/step - loss: 955790.2500 - mae: 955790.2500 - val_loss: 983001.1250 - val_mae: 983001.1250 - lr: 0.0010\n",
      "Epoch 295/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 949506.0625 - mae: 949506.0625 - val_loss: 980071.3125 - val_mae: 980071.3125 - lr: 0.0010\n",
      "Epoch 296/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 947497.8750 - mae: 947497.8750 - val_loss: 980463.5000 - val_mae: 980463.5000 - lr: 0.0010\n",
      "Epoch 297/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 957052.9375 - mae: 957052.9375 - val_loss: 983770.7500 - val_mae: 983770.7500 - lr: 0.0010\n",
      "Epoch 298/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 941848.3750 - mae: 941848.3750 - val_loss: 981292.7500 - val_mae: 981292.7500 - lr: 0.0010\n",
      "Epoch 299/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 934893.8750 - mae: 934893.8750 - val_loss: 980156.6250 - val_mae: 980156.6250 - lr: 0.0010\n",
      "Epoch 300/2000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 915610.3125 - mae: 915610.3125 - val_loss: 974982.8750 - val_mae: 974982.8750 - lr: 0.0010\n",
      "Epoch 301/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 862619.5625 - mae: 862619.5625 - val_loss: 968784.2500 - val_mae: 968784.2500 - lr: 0.0010\n",
      "Epoch 302/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 951254.4375 - mae: 951254.4375 - val_loss: 966825.0000 - val_mae: 966825.0000 - lr: 0.0010\n",
      "Epoch 303/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 959378.8750 - mae: 959378.8750 - val_loss: 972111.5000 - val_mae: 972111.5000 - lr: 0.0010\n",
      "Epoch 304/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 963556.6875 - mae: 963556.6875 - val_loss: 972010.3750 - val_mae: 972010.3750 - lr: 0.0010\n",
      "Epoch 305/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 969692.2500 - mae: 969692.2500 - val_loss: 973954.9375 - val_mae: 973954.9375 - lr: 0.0010\n",
      "Epoch 306/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 993508.4375 - mae: 993508.4375 - val_loss: 973333.3125 - val_mae: 973333.3125 - lr: 0.0010\n",
      "Epoch 307/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 969461.4375 - mae: 969461.4375 - val_loss: 980366.5000 - val_mae: 980366.5000 - lr: 0.0010\n",
      "Epoch 308/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 947147.8125 - mae: 947147.8125 - val_loss: 986302.6875 - val_mae: 986302.6875 - lr: 0.0010\n",
      "Epoch 309/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 906860.8750 - mae: 906860.8750 - val_loss: 987703.8125 - val_mae: 987703.8125 - lr: 0.0010\n",
      "Epoch 310/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 949384.2500 - mae: 949384.2500 - val_loss: 984113.5000 - val_mae: 984113.5000 - lr: 0.0010\n",
      "Epoch 311/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 966647.2500 - mae: 966647.2500 - val_loss: 984881.1875 - val_mae: 984881.1875 - lr: 0.0010\n",
      "Epoch 312/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1019351.9375 - mae: 1019351.9375 - val_loss: 980257.8750 - val_mae: 980257.8750 - lr: 0.0010\n",
      "Epoch 313/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 992141.7500 - mae: 992141.7500 - val_loss: 983127.1250 - val_mae: 983127.1250 - lr: 0.0010\n",
      "Epoch 314/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 938444.2500 - mae: 938444.2500 - val_loss: 983689.0000 - val_mae: 983689.0000 - lr: 0.0010\n",
      "Epoch 315/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 977486.2500 - mae: 977486.2500 - val_loss: 980154.1875 - val_mae: 980154.1875 - lr: 0.0010\n",
      "Epoch 316/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 943055.6250 - mae: 943055.6250 - val_loss: 980495.8750 - val_mae: 980495.8750 - lr: 0.0010\n",
      "Epoch 317/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 953904.8125 - mae: 953904.8125 - val_loss: 975677.2500 - val_mae: 975677.2500 - lr: 0.0010\n",
      "Epoch 318/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 977805.0000 - mae: 977805.0000 - val_loss: 972149.1875 - val_mae: 972149.1875 - lr: 0.0010\n",
      "Epoch 319/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1011561.0000 - mae: 1011561.0000 - val_loss: 970694.5000 - val_mae: 970694.5000 - lr: 0.0010\n",
      "Epoch 320/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 970346.9375 - mae: 970346.9375 - val_loss: 970977.4375 - val_mae: 970977.4375 - lr: 0.0010\n",
      "Epoch 321/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 902031.8125 - mae: 902031.8125 - val_loss: 974098.0625 - val_mae: 974098.0625 - lr: 0.0010\n",
      "Epoch 322/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 947389.6250 - mae: 947389.6250 - val_loss: 970032.0000 - val_mae: 970032.0000 - lr: 0.0010\n",
      "Epoch 323/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1002441.0000 - mae: 1002441.0000 - val_loss: 966182.1250 - val_mae: 966182.1250 - lr: 0.0010\n",
      "Epoch 324/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 940789.6250 - mae: 940789.6250 - val_loss: 967670.5000 - val_mae: 967670.5000 - lr: 0.0010\n",
      "Epoch 325/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 962498.2500 - mae: 962498.2500 - val_loss: 967327.3750 - val_mae: 967327.3750 - lr: 0.0010\n",
      "Epoch 326/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 899661.5625 - mae: 899661.5625 - val_loss: 968779.1250 - val_mae: 968779.1250 - lr: 0.0010\n",
      "Epoch 327/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 972044.7500 - mae: 972044.7500 - val_loss: 970920.1875 - val_mae: 970920.1875 - lr: 0.0010\n",
      "Epoch 328/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 936324.1875 - mae: 936324.1875 - val_loss: 981548.5625 - val_mae: 981548.5625 - lr: 0.0010\n",
      "Epoch 329/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 923096.6875 - mae: 923096.6875 - val_loss: 987756.5625 - val_mae: 987756.5625 - lr: 0.0010\n",
      "Epoch 330/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 957763.8750 - mae: 957763.8750 - val_loss: 973449.7500 - val_mae: 973449.7500 - lr: 0.0010\n",
      "Epoch 331/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 910377.9375 - mae: 910377.9375 - val_loss: 972122.6875 - val_mae: 972122.6875 - lr: 0.0010\n",
      "Epoch 332/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 986374.1250 - mae: 986374.1250 - val_loss: 977945.8125 - val_mae: 977945.8125 - lr: 0.0010\n",
      "Epoch 333/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 964221.0625 - mae: 964221.0625 - val_loss: 976150.5000 - val_mae: 976150.5000 - lr: 0.0010\n",
      "Epoch 334/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 933174.0625 - mae: 933174.0625 - val_loss: 979594.7500 - val_mae: 979594.7500 - lr: 0.0010\n",
      "Epoch 335/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 991680.5625 - mae: 991680.5625 - val_loss: 969127.9375 - val_mae: 969127.9375 - lr: 0.0010\n",
      "Epoch 336/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 905673.8125 - mae: 905673.8125 - val_loss: 968776.6250 - val_mae: 968776.6250 - lr: 0.0010\n",
      "Epoch 337/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 927670.1250 - mae: 927670.1250 - val_loss: 973922.7500 - val_mae: 973922.7500 - lr: 0.0010\n",
      "Epoch 338/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 968748.7500 - mae: 968748.7500 - val_loss: 981009.3125 - val_mae: 981009.3125 - lr: 0.0010\n",
      "Epoch 339/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 900267.9375 - mae: 900267.9375 - val_loss: 978723.6875 - val_mae: 978723.6875 - lr: 0.0010\n",
      "Epoch 340/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 958873.7500 - mae: 958873.7500 - val_loss: 973594.8125 - val_mae: 973594.8125 - lr: 0.0010\n",
      "Epoch 341/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 982857.0000 - mae: 982857.0000 - val_loss: 976362.0625 - val_mae: 976362.0625 - lr: 0.0010\n",
      "Epoch 342/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 921068.5000 - mae: 921068.5000 - val_loss: 972754.6250 - val_mae: 972754.6250 - lr: 0.0010\n",
      "Epoch 343/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 911412.8125 - mae: 911412.8125 - val_loss: 973792.9375 - val_mae: 973792.9375 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 930272.3750 - mae: 930272.3750 - val_loss: 968256.4375 - val_mae: 968256.4375 - lr: 0.0010\n",
      "Epoch 345/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 941530.4375 - mae: 941530.4375 - val_loss: 969283.2500 - val_mae: 969283.2500 - lr: 0.0010\n",
      "Epoch 346/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 946608.9375 - mae: 946608.9375 - val_loss: 974269.7500 - val_mae: 974269.7500 - lr: 0.0010\n",
      "Epoch 347/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 950390.0625 - mae: 950390.0625 - val_loss: 967714.1250 - val_mae: 967714.1250 - lr: 0.0010\n",
      "Epoch 348/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 939680.2500 - mae: 939680.2500 - val_loss: 967019.6250 - val_mae: 967019.6250 - lr: 0.0010\n",
      "Epoch 349/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 956143.6250 - mae: 956143.6250 - val_loss: 962603.0000 - val_mae: 962603.0000 - lr: 0.0010\n",
      "Epoch 350/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 881889.3125 - mae: 881889.3125 - val_loss: 970562.3750 - val_mae: 970562.3750 - lr: 0.0010\n",
      "Epoch 351/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 920088.8125 - mae: 920088.8125 - val_loss: 967593.3750 - val_mae: 967593.3750 - lr: 0.0010\n",
      "Epoch 352/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 961450.5000 - mae: 961450.5000 - val_loss: 957345.0000 - val_mae: 957345.0000 - lr: 0.0010\n",
      "Epoch 353/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 947785.5625 - mae: 947785.5625 - val_loss: 962032.7500 - val_mae: 962032.7500 - lr: 0.0010\n",
      "Epoch 354/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 893818.6250 - mae: 893818.6250 - val_loss: 976600.6250 - val_mae: 976600.6250 - lr: 0.0010\n",
      "Epoch 355/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 925684.3125 - mae: 925684.3125 - val_loss: 969895.8125 - val_mae: 969895.8125 - lr: 0.0010\n",
      "Epoch 356/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 976637.5000 - mae: 976637.5000 - val_loss: 967536.8750 - val_mae: 967536.8750 - lr: 0.0010\n",
      "Epoch 357/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 965951.1875 - mae: 965951.1875 - val_loss: 968661.8125 - val_mae: 968661.8125 - lr: 0.0010\n",
      "Epoch 358/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 915616.5000 - mae: 915616.5000 - val_loss: 980347.0000 - val_mae: 980347.0000 - lr: 0.0010\n",
      "Epoch 359/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 938787.8125 - mae: 938787.8125 - val_loss: 971292.7500 - val_mae: 971292.7500 - lr: 0.0010\n",
      "Epoch 360/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 962395.3125 - mae: 962395.3125 - val_loss: 972816.4375 - val_mae: 972816.4375 - lr: 0.0010\n",
      "Epoch 361/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 934437.3125 - mae: 934437.3125 - val_loss: 975654.8750 - val_mae: 975654.8750 - lr: 0.0010\n",
      "Epoch 362/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 957809.6250 - mae: 957809.6250 - val_loss: 972675.8125 - val_mae: 972675.8125 - lr: 0.0010\n",
      "Epoch 363/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 922817.3750 - mae: 922817.3750 - val_loss: 968762.3125 - val_mae: 968762.3125 - lr: 0.0010\n",
      "Epoch 364/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 960455.0625 - mae: 960455.0625 - val_loss: 962428.6875 - val_mae: 962428.6875 - lr: 0.0010\n",
      "Epoch 365/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 922299.6250 - mae: 922299.6250 - val_loss: 969825.0625 - val_mae: 969825.0625 - lr: 0.0010\n",
      "Epoch 366/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 960519.8750 - mae: 960519.8750 - val_loss: 963034.7500 - val_mae: 963034.7500 - lr: 0.0010\n",
      "Epoch 367/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 926548.1875 - mae: 926548.1875 - val_loss: 963390.1250 - val_mae: 963390.1250 - lr: 0.0010\n",
      "Epoch 368/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 918848.8125 - mae: 918848.8125 - val_loss: 981553.3125 - val_mae: 981553.3125 - lr: 0.0010\n",
      "Epoch 369/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 931186.1250 - mae: 931186.1250 - val_loss: 969728.9375 - val_mae: 969728.9375 - lr: 0.0010\n",
      "Epoch 370/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 907845.8125 - mae: 907845.8125 - val_loss: 968226.7500 - val_mae: 968226.7500 - lr: 0.0010\n",
      "Epoch 371/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 963656.1875 - mae: 963656.1875 - val_loss: 959808.0625 - val_mae: 959808.0625 - lr: 0.0010\n",
      "Epoch 372/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 905539.8750 - mae: 905539.8750 - val_loss: 967433.0625 - val_mae: 967433.0625 - lr: 0.0010\n",
      "Epoch 373/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 991230.1875 - mae: 991230.1875 - val_loss: 958161.8750 - val_mae: 958161.8750 - lr: 0.0010\n",
      "Epoch 374/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 976615.8750 - mae: 976615.8750 - val_loss: 961801.5000 - val_mae: 961801.5000 - lr: 0.0010\n",
      "Epoch 375/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 882691.7500 - mae: 882691.7500 - val_loss: 954983.2500 - val_mae: 954983.2500 - lr: 0.0010\n",
      "Epoch 376/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 950291.5625 - mae: 950291.5625 - val_loss: 947380.8125 - val_mae: 947380.8125 - lr: 0.0010\n",
      "Epoch 377/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1031344.7500 - mae: 1031344.7500 - val_loss: 945116.5625 - val_mae: 945116.5625 - lr: 0.0010\n",
      "Epoch 378/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 905022.9375 - mae: 905022.9375 - val_loss: 953924.3750 - val_mae: 953924.3750 - lr: 0.0010\n",
      "Epoch 379/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 896174.8125 - mae: 896174.8125 - val_loss: 955647.3750 - val_mae: 955647.3750 - lr: 0.0010\n",
      "Epoch 380/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1008001.6875 - mae: 1008001.6875 - val_loss: 952203.5625 - val_mae: 952203.5625 - lr: 0.0010\n",
      "Epoch 381/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 916735.0625 - mae: 916735.0625 - val_loss: 956905.9375 - val_mae: 956905.9375 - lr: 0.0010\n",
      "Epoch 382/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 914186.3750 - mae: 914186.3750 - val_loss: 958742.7500 - val_mae: 958742.7500 - lr: 0.0010\n",
      "Epoch 383/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 942718.1875 - mae: 942718.1875 - val_loss: 953232.7500 - val_mae: 953232.7500 - lr: 0.0010\n",
      "Epoch 384/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 954951.0625 - mae: 954951.0625 - val_loss: 960329.2500 - val_mae: 960329.2500 - lr: 0.0010\n",
      "Epoch 385/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 982203.8125 - mae: 982203.8125 - val_loss: 956269.5625 - val_mae: 956269.5625 - lr: 0.0010\n",
      "Epoch 386/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1004001.2500 - mae: 1004001.2500 - val_loss: 952341.0000 - val_mae: 952341.0000 - lr: 0.0010\n",
      "Epoch 387/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 959780.3125 - mae: 959780.3125 - val_loss: 960444.0625 - val_mae: 960444.0625 - lr: 0.0010\n",
      "Epoch 388/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 944126.3750 - mae: 944126.3750 - val_loss: 964212.2500 - val_mae: 964212.2500 - lr: 0.0010\n",
      "Epoch 389/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 953139.3125 - mae: 953139.3125 - val_loss: 975598.1250 - val_mae: 975598.1250 - lr: 0.0010\n",
      "Epoch 390/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 971273.6875 - mae: 971273.6875 - val_loss: 965027.0625 - val_mae: 965027.0625 - lr: 0.0010\n",
      "Epoch 391/2000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 973857.3750 - mae: 973857.3750 - val_loss: 961788.5625 - val_mae: 961788.5625 - lr: 0.0010\n",
      "Epoch 392/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 934751.6875 - mae: 934751.6875 - val_loss: 955356.0625 - val_mae: 955356.0625 - lr: 0.0010\n",
      "Epoch 393/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 921153.0000 - mae: 921153.0000 - val_loss: 961243.8750 - val_mae: 961243.8750 - lr: 0.0010\n",
      "Epoch 394/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 942905.3125 - mae: 942905.3125 - val_loss: 965457.4375 - val_mae: 965457.4375 - lr: 0.0010\n",
      "Epoch 395/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 874251.3750 - mae: 874251.3750 - val_loss: 976011.3125 - val_mae: 976011.3125 - lr: 0.0010\n",
      "Epoch 396/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 912642.7500 - mae: 912642.7500 - val_loss: 974372.9375 - val_mae: 974372.9375 - lr: 0.0010\n",
      "Epoch 397/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 983185.7500 - mae: 983185.7500 - val_loss: 973553.1875 - val_mae: 973553.1875 - lr: 0.0010\n",
      "Epoch 398/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 946097.0625 - mae: 946097.0625 - val_loss: 958604.5000 - val_mae: 958604.5000 - lr: 0.0010\n",
      "Epoch 399/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 916242.0000 - mae: 916242.0000 - val_loss: 961431.1250 - val_mae: 961431.1250 - lr: 0.0010\n",
      "Epoch 400/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 927734.8750 - mae: 927734.8750 - val_loss: 954884.5000 - val_mae: 954884.5000 - lr: 0.0010\n",
      "Epoch 401/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 943347.3750 - mae: 943347.3750 - val_loss: 957958.8750 - val_mae: 957958.8750 - lr: 0.0010\n",
      "Epoch 402/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 932899.3750 - mae: 932899.3750 - val_loss: 938237.6875 - val_mae: 938237.6875 - lr: 0.0010\n",
      "Epoch 403/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 954894.1875 - mae: 954894.1875 - val_loss: 932460.1250 - val_mae: 932460.1250 - lr: 0.0010\n",
      "Epoch 404/2000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 899801.8125 - mae: 899801.8125 - val_loss: 930517.5625 - val_mae: 930517.5625 - lr: 0.0010\n",
      "Epoch 405/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 954984.6875 - mae: 954984.6875 - val_loss: 928094.6250 - val_mae: 928094.6250 - lr: 0.0010\n",
      "Epoch 406/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 950570.1875 - mae: 950570.1875 - val_loss: 928345.8125 - val_mae: 928345.8125 - lr: 0.0010\n",
      "Epoch 407/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 934300.0000 - mae: 934300.0000 - val_loss: 926412.3750 - val_mae: 926412.3750 - lr: 0.0010\n",
      "Epoch 408/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 935415.8125 - mae: 935415.8125 - val_loss: 930997.6875 - val_mae: 930997.6875 - lr: 0.0010\n",
      "Epoch 409/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 910057.9375 - mae: 910057.9375 - val_loss: 936094.7500 - val_mae: 936094.7500 - lr: 0.0010\n",
      "Epoch 410/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 923851.0000 - mae: 923851.0000 - val_loss: 933197.6875 - val_mae: 933197.6875 - lr: 0.0010\n",
      "Epoch 411/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 964109.3125 - mae: 964109.3125 - val_loss: 937816.4375 - val_mae: 937816.4375 - lr: 0.0010\n",
      "Epoch 412/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 986101.7500 - mae: 986101.7500 - val_loss: 936079.6250 - val_mae: 936079.6250 - lr: 0.0010\n",
      "Epoch 413/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 959616.4375 - mae: 959616.4375 - val_loss: 934313.3750 - val_mae: 934313.3750 - lr: 0.0010\n",
      "Epoch 414/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 963392.8125 - mae: 963392.8125 - val_loss: 932689.3750 - val_mae: 932689.3750 - lr: 0.0010\n",
      "Epoch 415/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 948438.3750 - mae: 948438.3750 - val_loss: 938948.8750 - val_mae: 938948.8750 - lr: 0.0010\n",
      "Epoch 416/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 888402.4375 - mae: 888402.4375 - val_loss: 941020.3125 - val_mae: 941020.3125 - lr: 0.0010\n",
      "Epoch 417/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 908316.2500 - mae: 908316.2500 - val_loss: 948102.1250 - val_mae: 948102.1250 - lr: 0.0010\n",
      "Epoch 418/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 981613.5625 - mae: 981613.5625 - val_loss: 940448.9375 - val_mae: 940448.9375 - lr: 0.0010\n",
      "Epoch 419/2000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 899319.8125 - mae: 899319.8125 - val_loss: 946575.7500 - val_mae: 946575.7500 - lr: 0.0010\n",
      "Epoch 420/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 945188.8750 - mae: 945188.8750 - val_loss: 941397.0625 - val_mae: 941397.0625 - lr: 0.0010\n",
      "Epoch 421/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 932664.8750 - mae: 932664.8750 - val_loss: 935147.6250 - val_mae: 935147.6250 - lr: 0.0010\n",
      "Epoch 422/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 968147.1250 - mae: 968147.1250 - val_loss: 936191.0625 - val_mae: 936191.0625 - lr: 0.0010\n",
      "Epoch 423/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 943243.0625 - mae: 943243.0625 - val_loss: 934909.6250 - val_mae: 934909.6250 - lr: 0.0010\n",
      "Epoch 424/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 955077.1250 - mae: 955077.1250 - val_loss: 930238.4375 - val_mae: 930238.4375 - lr: 0.0010\n",
      "Epoch 425/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 943970.1875 - mae: 943970.1875 - val_loss: 933732.8750 - val_mae: 933732.8750 - lr: 0.0010\n",
      "Epoch 426/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 893507.6250 - mae: 893507.6250 - val_loss: 931439.2500 - val_mae: 931439.2500 - lr: 0.0010\n",
      "Epoch 427/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 914662.0625 - mae: 914662.0625 - val_loss: 932259.5000 - val_mae: 932259.5000 - lr: 0.0010\n",
      "Epoch 428/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 923309.3125 - mae: 923309.3125 - val_loss: 931872.6875 - val_mae: 931872.6875 - lr: 0.0010\n",
      "Epoch 429/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 933061.6875 - mae: 933061.6875 - val_loss: 937874.9375 - val_mae: 937874.9375 - lr: 0.0010\n",
      "Epoch 430/2000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 907692.8125 - mae: 907692.8125 - val_loss: 939157.1875 - val_mae: 939157.1875 - lr: 0.0010\n",
      "Epoch 431/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 915537.1875 - mae: 915537.1875 - val_loss: 936071.5625 - val_mae: 936071.5625 - lr: 0.0010\n",
      "Epoch 432/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 927754.0000 - mae: 927754.0000 - val_loss: 927927.8125 - val_mae: 927927.8125 - lr: 0.0010\n",
      "Epoch 433/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 905887.0000 - mae: 905887.0000 - val_loss: 937724.9375 - val_mae: 937724.9375 - lr: 0.0010\n",
      "Epoch 434/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 876544.6875 - mae: 876544.6875 - val_loss: 927682.3750 - val_mae: 927682.3750 - lr: 0.0010\n",
      "Epoch 435/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 897141.7500 - mae: 897141.7500 - val_loss: 929919.6250 - val_mae: 929919.6250 - lr: 0.0010\n",
      "Epoch 436/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 901948.1250 - mae: 901948.1250 - val_loss: 933585.7500 - val_mae: 933585.7500 - lr: 0.0010\n",
      "Epoch 437/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 942935.6875 - mae: 942935.6875 - val_loss: 931087.5000 - val_mae: 931087.5000 - lr: 0.0010\n",
      "Epoch 438/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 933849.6875 - mae: 933849.6875 - val_loss: 929872.0000 - val_mae: 929872.0000 - lr: 0.0010\n",
      "Epoch 439/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 917111.4375 - mae: 917111.4375 - val_loss: 929868.7500 - val_mae: 929868.7500 - lr: 0.0010\n",
      "Epoch 440/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 925494.8750 - mae: 925494.8750 - val_loss: 938367.7500 - val_mae: 938367.7500 - lr: 0.0010\n",
      "Epoch 441/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 861748.5000 - mae: 861748.5000 - val_loss: 960549.1875 - val_mae: 960549.1875 - lr: 0.0010\n",
      "Epoch 442/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 897332.4375 - mae: 897332.4375 - val_loss: 962286.0625 - val_mae: 962286.0625 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 900585.2500 - mae: 900585.2500 - val_loss: 962406.0625 - val_mae: 962406.0625 - lr: 0.0010\n",
      "Epoch 444/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 944896.3125 - mae: 944896.3125 - val_loss: 956718.8750 - val_mae: 956718.8750 - lr: 0.0010\n",
      "Epoch 445/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 933788.3750 - mae: 933788.3750 - val_loss: 959031.3750 - val_mae: 959031.3750 - lr: 0.0010\n",
      "Epoch 446/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 897348.1250 - mae: 897348.1250 - val_loss: 946953.3750 - val_mae: 946953.3750 - lr: 0.0010\n",
      "Epoch 447/2000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 927299.6875 - mae: 927299.6875 - val_loss: 943210.6875 - val_mae: 943210.6875 - lr: 0.0010\n",
      "Epoch 448/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 976694.5625 - mae: 976694.5625 - val_loss: 945879.3750 - val_mae: 945879.3750 - lr: 0.0010\n",
      "Epoch 449/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 920196.7500 - mae: 920196.7500 - val_loss: 952666.3125 - val_mae: 952666.3125 - lr: 0.0010\n",
      "Epoch 450/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 867805.6875 - mae: 867805.6875 - val_loss: 949308.6250 - val_mae: 949308.6250 - lr: 0.0010\n",
      "Epoch 451/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 932311.3750 - mae: 932311.3750 - val_loss: 954057.9375 - val_mae: 954057.9375 - lr: 0.0010\n",
      "Epoch 452/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 890043.8125 - mae: 890043.8125 - val_loss: 955793.3125 - val_mae: 955793.3125 - lr: 0.0010\n",
      "Epoch 453/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 931857.0625 - mae: 931857.0625 - val_loss: 952855.5625 - val_mae: 952855.5625 - lr: 0.0010\n",
      "Epoch 454/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 973930.5625 - mae: 973930.5625 - val_loss: 948476.5625 - val_mae: 948476.5625 - lr: 0.0010\n",
      "Epoch 455/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 881176.3750 - mae: 881176.3750 - val_loss: 945536.5000 - val_mae: 945536.5000 - lr: 0.0010\n",
      "Epoch 456/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 914720.0000 - mae: 914720.0000 - val_loss: 946569.6250 - val_mae: 946569.6250 - lr: 0.0010\n",
      "Epoch 457/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 945703.0000 - mae: 945703.0000 - val_loss: 940177.5625 - val_mae: 940177.5625 - lr: 0.0010\n",
      "Epoch 458/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 880219.3125 - mae: 880219.3125 - val_loss: 947201.8125 - val_mae: 947201.8125 - lr: 0.0010\n",
      "Epoch 459/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 915045.2500 - mae: 915045.2500 - val_loss: 937381.8125 - val_mae: 937381.8125 - lr: 0.0010\n",
      "Epoch 460/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 990094.8125 - mae: 990094.8125 - val_loss: 933853.8125 - val_mae: 933853.8125 - lr: 0.0010\n",
      "Epoch 461/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 930703.1875 - mae: 930703.1875 - val_loss: 933985.3125 - val_mae: 933985.3125 - lr: 0.0010\n",
      "Epoch 462/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 976683.0625 - mae: 976683.0625 - val_loss: 939385.5625 - val_mae: 939385.5625 - lr: 0.0010\n",
      "Epoch 463/2000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 957933.1875 - mae: 957933.1875 - val_loss: 943786.5625 - val_mae: 943786.5625 - lr: 0.0010\n",
      "Epoch 464/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 932855.0000 - mae: 932855.0000 - val_loss: 954167.8750 - val_mae: 954167.8750 - lr: 0.0010\n",
      "Epoch 465/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 968194.6250 - mae: 968194.6250 - val_loss: 961028.4375 - val_mae: 961028.4375 - lr: 0.0010\n",
      "Epoch 466/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 905145.0000 - mae: 905145.0000 - val_loss: 959781.5625 - val_mae: 959781.5625 - lr: 0.0010\n",
      "Epoch 467/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 936637.5625 - mae: 936637.5625 - val_loss: 964681.3750 - val_mae: 964681.3750 - lr: 0.0010\n",
      "Epoch 468/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 890784.2500 - mae: 890784.2500 - val_loss: 963796.6250 - val_mae: 963796.6250 - lr: 0.0010\n",
      "Epoch 469/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 935404.0625 - mae: 935404.0625 - val_loss: 953895.8125 - val_mae: 953895.8125 - lr: 0.0010\n",
      "Epoch 470/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 931505.4375 - mae: 931505.4375 - val_loss: 955135.7500 - val_mae: 955135.7500 - lr: 0.0010\n",
      "Epoch 471/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 900852.5000 - mae: 900852.5000 - val_loss: 961585.0625 - val_mae: 961585.0625 - lr: 0.0010\n",
      "Epoch 472/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 885146.6250 - mae: 885146.6250 - val_loss: 974315.1250 - val_mae: 974315.1250 - lr: 0.0010\n",
      "Epoch 473/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 920956.3125 - mae: 920956.3125 - val_loss: 957923.8125 - val_mae: 957923.8125 - lr: 0.0010\n",
      "Epoch 474/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 889614.5000 - mae: 889614.5000 - val_loss: 956571.1875 - val_mae: 956571.1875 - lr: 0.0010\n",
      "Epoch 475/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 946586.4375 - mae: 946586.4375 - val_loss: 959041.8750 - val_mae: 959041.8750 - lr: 0.0010\n",
      "Epoch 476/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 935127.8750 - mae: 935127.8750 - val_loss: 959661.3125 - val_mae: 959661.3125 - lr: 0.0010\n",
      "Epoch 477/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 916972.2500 - mae: 916972.2500 - val_loss: 960659.6875 - val_mae: 960659.6875 - lr: 0.0010\n",
      "Epoch 478/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 906911.2500 - mae: 906911.2500 - val_loss: 959254.3750 - val_mae: 959254.3750 - lr: 0.0010\n",
      "Epoch 479/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 948772.5625 - mae: 948772.5625 - val_loss: 945169.9375 - val_mae: 945169.9375 - lr: 0.0010\n",
      "Epoch 480/2000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 928722.5625 - mae: 928722.5625 - val_loss: 943528.0625 - val_mae: 943528.0625 - lr: 0.0010\n",
      "Epoch 481/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 937702.3750 - mae: 937702.3750 - val_loss: 951335.0000 - val_mae: 951335.0000 - lr: 0.0010\n",
      "Epoch 482/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 932663.0625 - mae: 932663.0625 - val_loss: 953256.6250 - val_mae: 953256.6250 - lr: 0.0010\n",
      "Epoch 483/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 933452.5000 - mae: 933452.5000 - val_loss: 957149.0625 - val_mae: 957149.0625 - lr: 0.0010\n",
      "Epoch 484/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 873269.3125 - mae: 873269.3125 - val_loss: 957456.9375 - val_mae: 957456.9375 - lr: 0.0010\n",
      "Epoch 485/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 926841.9375 - mae: 926841.9375 - val_loss: 970567.6250 - val_mae: 970567.6250 - lr: 0.0010\n",
      "Epoch 486/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 989488.7500 - mae: 989488.7500 - val_loss: 964984.7500 - val_mae: 964984.7500 - lr: 0.0010\n",
      "Epoch 487/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 867212.3125 - mae: 867212.3125 - val_loss: 970882.6250 - val_mae: 970882.6250 - lr: 0.0010\n",
      "Epoch 488/2000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 901547.6250 - mae: 901547.6250 - val_loss: 984530.2500 - val_mae: 984530.2500 - lr: 0.0010\n",
      "Epoch 489/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 939035.1875 - mae: 939035.1875 - val_loss: 968146.2500 - val_mae: 968146.2500 - lr: 0.0010\n",
      "Epoch 490/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 946721.6250 - mae: 946721.6250 - val_loss: 962352.9375 - val_mae: 962352.9375 - lr: 0.0010\n",
      "Epoch 491/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 965172.9375 - mae: 965172.9375 - val_loss: 958996.3750 - val_mae: 958996.3750 - lr: 0.0010\n",
      "Epoch 492/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 923041.3125 - mae: 923041.3125 - val_loss: 954007.2500 - val_mae: 954007.2500 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 919931.6875 - mae: 919931.6875 - val_loss: 954087.0000 - val_mae: 954087.0000 - lr: 0.0010\n",
      "Epoch 494/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 945708.4375 - mae: 945708.4375 - val_loss: 955474.6250 - val_mae: 955474.6250 - lr: 0.0010\n",
      "Epoch 495/2000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 909065.5625 - mae: 909065.5625 - val_loss: 956701.0625 - val_mae: 956701.0625 - lr: 0.0010\n",
      "Epoch 496/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 933247.0000 - mae: 933247.0000 - val_loss: 964128.8125 - val_mae: 964128.8125 - lr: 0.0010\n",
      "Epoch 497/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 894150.6250 - mae: 894150.6250 - val_loss: 960959.8750 - val_mae: 960959.8750 - lr: 0.0010\n",
      "Epoch 498/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 965828.3125 - mae: 965828.3125 - val_loss: 958681.0000 - val_mae: 958681.0000 - lr: 0.0010\n",
      "Epoch 499/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 962361.7500 - mae: 962361.7500 - val_loss: 958170.4375 - val_mae: 958170.4375 - lr: 0.0010\n",
      "Epoch 500/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 912864.0000 - mae: 912864.0000 - val_loss: 953503.8750 - val_mae: 953503.8750 - lr: 0.0010\n",
      "Epoch 501/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 915957.6250 - mae: 915957.6250 - val_loss: 951040.2500 - val_mae: 951040.2500 - lr: 0.0010\n",
      "Epoch 502/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 953704.8750 - mae: 953704.8750 - val_loss: 947931.8125 - val_mae: 947931.8125 - lr: 0.0010\n",
      "Epoch 503/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 896430.6875 - mae: 896430.6875 - val_loss: 948057.0000 - val_mae: 948057.0000 - lr: 0.0010\n",
      "Epoch 504/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 912024.5625 - mae: 912024.5625 - val_loss: 959345.8750 - val_mae: 959345.8750 - lr: 0.0010\n",
      "Epoch 505/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 905181.8125 - mae: 905181.8125 - val_loss: 962582.0625 - val_mae: 962582.0625 - lr: 0.0010\n",
      "Epoch 506/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 872160.3125 - mae: 872160.3125 - val_loss: 956601.0000 - val_mae: 956601.0000 - lr: 0.0010\n",
      "Epoch 507/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 948746.2500 - mae: 948746.2500 - val_loss: 948546.6875 - val_mae: 948546.6875 - lr: 0.0010\n",
      "Epoch 508/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 921670.0000 - mae: 921670.0000 - val_loss: 947385.8125 - val_mae: 947385.8125 - lr: 2.0000e-04\n",
      "Epoch 509/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 924065.5000 - mae: 924065.5000 - val_loss: 947447.9375 - val_mae: 947447.9375 - lr: 2.0000e-04\n",
      "Epoch 510/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 951168.4375 - mae: 951168.4375 - val_loss: 945861.0000 - val_mae: 945861.0000 - lr: 2.0000e-04\n",
      "Epoch 511/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 965635.6250 - mae: 965635.6250 - val_loss: 945211.1250 - val_mae: 945211.1250 - lr: 2.0000e-04\n",
      "Epoch 512/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 937290.8125 - mae: 937290.8125 - val_loss: 944372.7500 - val_mae: 944372.7500 - lr: 2.0000e-04\n",
      "Epoch 513/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 951149.1250 - mae: 951149.1250 - val_loss: 942812.6250 - val_mae: 942812.6250 - lr: 2.0000e-04\n",
      "Epoch 514/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 910515.1250 - mae: 910515.1250 - val_loss: 943461.4375 - val_mae: 943461.4375 - lr: 2.0000e-04\n",
      "Epoch 515/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 865212.0625 - mae: 865212.0625 - val_loss: 942133.9375 - val_mae: 942133.9375 - lr: 2.0000e-04\n",
      "Epoch 516/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 872408.1250 - mae: 872408.1250 - val_loss: 942983.5625 - val_mae: 942983.5625 - lr: 2.0000e-04\n",
      "Epoch 517/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 978506.5000 - mae: 978506.5000 - val_loss: 942932.5000 - val_mae: 942932.5000 - lr: 2.0000e-04\n",
      "Epoch 518/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 900392.3750 - mae: 900392.3750 - val_loss: 942048.1250 - val_mae: 942048.1250 - lr: 2.0000e-04\n",
      "Epoch 519/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 929534.5000 - mae: 929534.5000 - val_loss: 942490.1875 - val_mae: 942490.1875 - lr: 2.0000e-04\n",
      "Epoch 520/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 863512.6875 - mae: 863512.6875 - val_loss: 942612.8750 - val_mae: 942612.8750 - lr: 2.0000e-04\n",
      "Epoch 521/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 858437.4375 - mae: 858437.4375 - val_loss: 942123.2500 - val_mae: 942123.2500 - lr: 2.0000e-04\n",
      "Epoch 522/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 931108.8125 - mae: 931108.8125 - val_loss: 941274.3750 - val_mae: 941274.3750 - lr: 2.0000e-04\n",
      "Epoch 523/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 932273.4375 - mae: 932273.4375 - val_loss: 942287.7500 - val_mae: 942287.7500 - lr: 2.0000e-04\n",
      "Epoch 524/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 856973.5625 - mae: 856973.5625 - val_loss: 944427.1250 - val_mae: 944427.1250 - lr: 2.0000e-04\n",
      "Epoch 525/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 917055.5000 - mae: 917055.5000 - val_loss: 944012.5625 - val_mae: 944012.5625 - lr: 2.0000e-04\n",
      "Epoch 526/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 913025.0000 - mae: 913025.0000 - val_loss: 944729.0000 - val_mae: 944729.0000 - lr: 2.0000e-04\n",
      "Epoch 527/2000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 916507.1250 - mae: 916507.1250 - val_loss: 943315.8750 - val_mae: 943315.8750 - lr: 2.0000e-04\n",
      "Epoch 528/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 941256.6250 - mae: 941256.6250 - val_loss: 943085.5000 - val_mae: 943085.5000 - lr: 2.0000e-04\n",
      "Epoch 529/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 920816.0625 - mae: 920816.0625 - val_loss: 942419.0000 - val_mae: 942419.0000 - lr: 2.0000e-04\n",
      "Epoch 530/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 882789.2500 - mae: 882789.2500 - val_loss: 946186.6875 - val_mae: 946186.6875 - lr: 2.0000e-04\n",
      "Epoch 531/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 916735.3125 - mae: 916735.3125 - val_loss: 947250.6250 - val_mae: 947250.6250 - lr: 2.0000e-04\n",
      "Epoch 532/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 936653.8750 - mae: 936653.9375 - val_loss: 947366.5000 - val_mae: 947366.5000 - lr: 2.0000e-04\n",
      "Epoch 533/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 968319.0625 - mae: 968319.0625 - val_loss: 944523.0000 - val_mae: 944523.0000 - lr: 2.0000e-04\n",
      "Epoch 534/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 992316.5625 - mae: 992316.5625 - val_loss: 944561.5625 - val_mae: 944561.5625 - lr: 2.0000e-04\n",
      "Epoch 535/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 959481.6250 - mae: 959481.6250 - val_loss: 943824.5000 - val_mae: 943824.5000 - lr: 2.0000e-04\n",
      "Epoch 536/2000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 901499.5625 - mae: 901499.5625 - val_loss: 943719.6250 - val_mae: 943719.6250 - lr: 2.0000e-04\n",
      "Epoch 537/2000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 890732.8125 - mae: 890732.8125 - val_loss: 941558.5625 - val_mae: 941558.5625 - lr: 2.0000e-04\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1065719.8750 - mae: 1065719.8750\n",
      "\n",
      "model mae: 1065719.8750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.3, epochs=2000, batch_size=64, verbose=1, callbacks=[early_stopping_callback,checkpointer,reduce_lr])\n",
    "print(\"\\nmodel mae: %.4f\" % (model.evaluate(X_test, y_test)[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270    3364079.0\n",
       "267     837500.0\n",
       "449     414626.0\n",
       "357     900054.0\n",
       "189     818384.0\n",
       "         ...    \n",
       "163       3427.0\n",
       "20      951797.0\n",
       "543      22893.0\n",
       "382    2471721.0\n",
       "348     611685.0\n",
       "Name: Target, Length: 173, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3292983.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction[171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978440.9961001005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 829044.0625 - mae: 829044.0625\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 930708.3125 - mae: 930708.3125\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 910013.6875 - mae: 910013.6875\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 878613.3125 - mae: 878613.3125\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 968192.0000 - mae: 968192.0000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 937767.2500 - mae: 937767.2500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 797785.1250 - mae: 797785.1250\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 940954.0625 - mae: 940954.0625\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 883183.1875 - mae: 883183.1875\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 873095.0000 - mae: 873095.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "acc_list = []\n",
    "nn_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    model.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = model.predict(val_x)\n",
    "    sub_pred = model.predict(X_test).flatten()\n",
    "    mae= mean_absolute_error(y_test, Y_prediction)\n",
    "    \n",
    "    acc_list.append(mae)\n",
    "    \n",
    "    nn_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991764.4583196916"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, nn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1341555 16,rmsprop 0.2 20 40 1000 64\n",
    "1339283 16,Nadam 0.2 20 40 1000 64\n",
    "1316436 16,Nadam 0.2 20 40 1000 64\n",
    "1253338 8 0.5 16 0.5 32 0.5 32 0.5 16 0.5 8 Nadam/ 0.2 20 30/ 1000 64\n",
    "1058512 8 0.2 16 0.2 32 0.2 32 0.2 16 0.2 8 Nadam/ 0.2 20 30/ 1000 64\n",
    "1639713 8 batch 16 batch 32 batch 32 batch 16 batch 8 Nadam/ 0.2 20 30/ 1000 64\n",
    "1041020 8 0.2 16 0.2 32 0.2 32 0.2 16 0.2 8 Nadam/ 0.2 30 40/ 1000 64\n",
    "1034220 8 0.2 16 0.2 32 0.2 8 0.2 32 0.2 16 0.2 8 Nadam/ 0.2 30 40/ 1000 64\n",
    "1040655 8 0.2 16 0.2 32 0.2 8 0.2 32 0.2 16 0.2 8 Nadam/ 0.2 100 130/ 1000 64\n",
    "\n",
    "정규화 후\n",
    "1004068 8 0.2 16 0.2 32 0.2 8 0.2 32 0.2 16 0.2 8 adam/ 0.2 100 130/ 1000 64\n",
    "996210 8 0.2 16 0.2 32 0.2 8 0.2 32 0.2 16 0.2 8 Nadam/ 0.2 100 130/ 1000 64\n",
    "937944,0.973162,978440 8 0.2 16 0.2 32 0.2 8 0.2 32 0.2 16 0.2 8 rmsprop/ 0.2 100 130/ 1000 64\n",
    "960552.8125 8 0.2 16 0.2 32 0.2 8 0.2 32 rmsprop/ 0.2 100 130/ 1000 64\n",
    "974380 8 0.2 16 0.2 32 0.2 32 0.2 32 rmsprop/ 0.2 100 130/ 1000 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 1000, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_splits=5 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-264-e70b8487bc5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnn_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{i + 1} Fold Training.....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    334\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mmin_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0my_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             raise ValueError(\"n_splits=%d cannot be greater than the\"\n\u001b[0m\u001b[0;32m    667\u001b[0m                              \u001b[1;34m\" number of members in each class.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                              % (self.n_splits))\n",
      "\u001b[1;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y_test.shape[0],8))\n",
    "for i,(train, test) in enumerate(skf.split(X_train,y_train)):\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    \n",
    "    # 딥러닝 모델 선언\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=7, activation='elu'))  # 입력층\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='elu'))      # 은닉층1 \n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='elu'))      # 은닉층1\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='elu'))      # 은닉층1\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(16, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(8, activation='elu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(8, activation='elu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(Dense(8, activation='elu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dense(8, activation='elu'))\n",
    "    #model.add(Dense(4, activation='elu'))\n",
    "    model.add(Dense(1))# 출력층\n",
    "    # 선형 회귀는 마지막에 참과 거짓을 구분할 필요가 없음. 출력층에 활성화 함수를 지정할 필요도 없음\n",
    "\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',optimizer='rmsprop', metrics=['mae'])\n",
    "\n",
    "    # 모델 업데이트 및 저장\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_mae', mode = 'min', verbose = 0) #\n",
    "    # 콜백함수\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=100, mode='min')\n",
    "    # 학습 자동 중단 설정\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_mae', patience=130, mode='min')\n",
    "\n",
    "    #optimizer = Adam, RMSprop, Nadam, Adabelif Optimizer\n",
    "    history = model.fit(X_train.iloc[train],y_train.iloc[train], validation_split=0.3, epochs=1000, batch_size=64, verbose=0,callbacks=[early_stopping_callback,mc,reduce_lr]) \n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "\n",
    "    fold_nn_acc = model.evaluate(X_train.iloc[test], y_train.iloc[test])[1]        \n",
    "\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold mae of NN = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(X_test)/  skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화 안하는게 더 좋은 결과를 낳음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_fin, test_size=0.3, random_state=100)\n",
    "cat_features = ['genre_rank', 'direct_rank','dist_rank']  # 범주 변수들 설정해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_b9c487ba_d280_11ec_86cc_7085c2294e08row42_col1{\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Description</th>        <th class=\"col_heading level0 col1\" >Value</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row0_col1\" class=\"data row0 col1\" >8062</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row1_col1\" class=\"data row1 col1\" >Target</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row2_col0\" class=\"data row2 col0\" >Original Data</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row2_col1\" class=\"data row2 col1\" >(402, 8)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row3_col0\" class=\"data row3 col0\" >Missing Values</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row3_col1\" class=\"data row3 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row4_col0\" class=\"data row4 col0\" >Numeric Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row4_col1\" class=\"data row4 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row5_col0\" class=\"data row5 col0\" >Categorical Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row5_col1\" class=\"data row5 col1\" >3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row6_col0\" class=\"data row6 col0\" >Ordinal Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row6_col1\" class=\"data row6 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row7_col0\" class=\"data row7 col0\" >High Cardinality Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row7_col1\" class=\"data row7 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row8_col0\" class=\"data row8 col0\" >High Cardinality Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row8_col1\" class=\"data row8 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row9_col0\" class=\"data row9 col0\" >Transformed Train Set</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row9_col1\" class=\"data row9 col1\" >(281, 114)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row10_col0\" class=\"data row10 col0\" >Transformed Test Set</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row10_col1\" class=\"data row10 col1\" >(121, 114)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row11_col0\" class=\"data row11 col0\" >Shuffle Train-Test</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row11_col1\" class=\"data row11 col1\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row12_col0\" class=\"data row12 col0\" >Stratify Train-Test</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row12_col1\" class=\"data row12 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row13_col1\" class=\"data row13 col1\" >KFold</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row18_col1\" class=\"data row18 col1\" >reg-default-name</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row19_col1\" class=\"data row19 col1\" >64e1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row20_col0\" class=\"data row20 col0\" >Imputation Type</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row20_col1\" class=\"data row20 col1\" >simple</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row21_col0\" class=\"data row21 col0\" >Iterative Imputation Iteration</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row21_col1\" class=\"data row21 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row22_col0\" class=\"data row22 col0\" >Numeric Imputer</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row22_col1\" class=\"data row22 col1\" >mean</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row23_col0\" class=\"data row23 col0\" >Iterative Imputation Numeric Model</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row24_col0\" class=\"data row24 col0\" >Categorical Imputer</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row24_col1\" class=\"data row24 col1\" >constant</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row25_col0\" class=\"data row25 col0\" >Iterative Imputation Categorical Model</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row25_col1\" class=\"data row25 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row26_col0\" class=\"data row26 col0\" >Unknown Categoricals Handling</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row26_col1\" class=\"data row26 col1\" >least_frequent</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row27_col0\" class=\"data row27 col0\" >Normalize</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row27_col1\" class=\"data row27 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row28_col0\" class=\"data row28 col0\" >Normalize Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row28_col1\" class=\"data row28 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row29_col0\" class=\"data row29 col0\" >Transformation</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row29_col1\" class=\"data row29 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row30_col0\" class=\"data row30 col0\" >Transformation Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row30_col1\" class=\"data row30 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row31_col0\" class=\"data row31 col0\" >PCA</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row32_col0\" class=\"data row32 col0\" >PCA Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row32_col1\" class=\"data row32 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row33_col0\" class=\"data row33 col0\" >PCA Components</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row33_col1\" class=\"data row33 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row34_col0\" class=\"data row34 col0\" >Ignore Low Variance</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row34_col1\" class=\"data row34 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row35_col0\" class=\"data row35 col0\" >Combine Rare Levels</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row35_col1\" class=\"data row35 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row36_col0\" class=\"data row36 col0\" >Rare Level Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row36_col1\" class=\"data row36 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row37_col0\" class=\"data row37 col0\" >Numeric Binning</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row37_col1\" class=\"data row37 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row38_col0\" class=\"data row38 col0\" >Remove Outliers</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row38_col1\" class=\"data row38 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row39_col0\" class=\"data row39 col0\" >Outliers Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row39_col1\" class=\"data row39 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row40_col0\" class=\"data row40 col0\" >Remove Multicollinearity</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row40_col1\" class=\"data row40 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row41_col0\" class=\"data row41 col0\" >Multicollinearity Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row41_col1\" class=\"data row41 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row42_col0\" class=\"data row42 col0\" >Remove Perfect Collinearity</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row42_col1\" class=\"data row42 col1\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row43_col0\" class=\"data row43 col0\" >Clustering</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row43_col1\" class=\"data row43 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row44_col0\" class=\"data row44 col0\" >Clustering Iteration</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row44_col1\" class=\"data row44 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row45_col0\" class=\"data row45 col0\" >Polynomial Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row45_col1\" class=\"data row45 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row46_col0\" class=\"data row46 col0\" >Polynomial Degree</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row46_col1\" class=\"data row46 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row47_col0\" class=\"data row47 col0\" >Trignometry Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row47_col1\" class=\"data row47 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row48_col0\" class=\"data row48 col0\" >Polynomial Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row48_col1\" class=\"data row48 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row49_col0\" class=\"data row49 col0\" >Group Features</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row49_col1\" class=\"data row49 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row50_col0\" class=\"data row50 col0\" >Feature Selection</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row50_col1\" class=\"data row50 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row51_col0\" class=\"data row51 col0\" >Feature Selection Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row51_col1\" class=\"data row51 col1\" >classic</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row52_col0\" class=\"data row52 col0\" >Features Selection Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row52_col1\" class=\"data row52 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row53_col0\" class=\"data row53 col0\" >Feature Interaction</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row53_col1\" class=\"data row53 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row54_col0\" class=\"data row54 col0\" >Feature Ratio</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row54_col1\" class=\"data row54 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row55_col0\" class=\"data row55 col0\" >Interaction Threshold</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row55_col1\" class=\"data row55 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row56_col0\" class=\"data row56 col0\" >Transform Target</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row56_col1\" class=\"data row56 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row57_col0\" class=\"data row57 col0\" >Transform Target Method</td>\n",
       "                        <td id=\"T_b9c487ba_d280_11ec_86cc_7085c2294e08row57_col1\" class=\"data row57 col1\" >box-cox</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15a7cf58310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_clf = setup(data = train, target= 'Target', fold_shuffle=True, categorical_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_cd02590d_d280_11ec_967d_7085c2294e08 th {\n",
       "          text-align: left;\n",
       "    }#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col6,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col0,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col6{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "        }#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col2,#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col3,#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col4,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col1,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col5,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col6{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "        }#T_cd02590d_d280_11ec_967d_7085c2294e08row0_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row1_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row2_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row3_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row4_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row5_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row6_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row7_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row8_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row9_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row10_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row11_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row12_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row13_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row14_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row15_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row16_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row18_col7,#T_cd02590d_d280_11ec_967d_7085c2294e08row19_col7{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  lightgrey;\n",
       "        }#T_cd02590d_d280_11ec_967d_7085c2294e08row17_col7{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "            background-color:  lightgrey;\n",
       "        }</style><table id=\"T_cd02590d_d280_11ec_967d_7085c2294e08\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >MAE</th>        <th class=\"col_heading level0 col2\" >MSE</th>        <th class=\"col_heading level0 col3\" >RMSE</th>        <th class=\"col_heading level0 col4\" >R2</th>        <th class=\"col_heading level0 col5\" >RMSLE</th>        <th class=\"col_heading level0 col6\" >MAPE</th>        <th class=\"col_heading level0 col7\" >TT (Sec)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row0\" class=\"row_heading level0 row0\" >ridge</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col0\" class=\"data row0 col0\" >Ridge Regression</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col1\" class=\"data row0 col1\" >1249348.1500</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col2\" class=\"data row0 col2\" >4031297250918.3999</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col3\" class=\"data row0 col3\" >1986415.6250</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col4\" class=\"data row0 col4\" >0.4619</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col5\" class=\"data row0 col5\" >3.0455</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col6\" class=\"data row0 col6\" >158.7325</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row0_col7\" class=\"data row0 col7\" >0.0120</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row1\" class=\"row_heading level0 row1\" >en</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col0\" class=\"data row1 col0\" >Elastic Net</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col1\" class=\"data row1 col1\" >1210550.6000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col2\" class=\"data row1 col2\" >4172685980467.2002</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col3\" class=\"data row1 col3\" >2023363.1750</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col4\" class=\"data row1 col4\" >0.4429</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col5\" class=\"data row1 col5\" >2.8611</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col6\" class=\"data row1 col6\" >118.3336</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row1_col7\" class=\"data row1 col7\" >0.0180</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row2\" class=\"row_heading level0 row2\" >rf</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col0\" class=\"data row2 col0\" >Random Forest Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col1\" class=\"data row2 col1\" >1070719.5135</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col2\" class=\"data row2 col2\" >4203318835838.9463</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col3\" class=\"data row2 col3\" >2030313.6860</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col4\" class=\"data row2 col4\" >0.4332</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col5\" class=\"data row2 col5\" >1.5403</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col6\" class=\"data row2 col6\" >11.4691</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row2_col7\" class=\"data row2 col7\" >0.1500</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row3\" class=\"row_heading level0 row3\" >gbr</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col0\" class=\"data row3 col0\" >Gradient Boosting Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col1\" class=\"data row3 col1\" >1072954.9604</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col2\" class=\"data row3 col2\" >4306356245402.4897</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col3\" class=\"data row3 col3\" >2060174.7240</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col4\" class=\"data row3 col4\" >0.4157</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col5\" class=\"data row3 col5\" >2.2833</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col6\" class=\"data row3 col6\" >39.6001</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row3_col7\" class=\"data row3 col7\" >0.0420</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row4\" class=\"row_heading level0 row4\" >br</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col0\" class=\"data row4 col0\" >Bayesian Ridge</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col1\" class=\"data row4 col1\" >1212422.0319</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col2\" class=\"data row4 col2\" >4308619773003.8232</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col3\" class=\"data row4 col3\" >2051784.1355</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col4\" class=\"data row4 col4\" >0.4275</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col5\" class=\"data row4 col5\" >2.7954</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col6\" class=\"data row4 col6\" >94.1566</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row4_col7\" class=\"data row4 col7\" >0.0100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row5\" class=\"row_heading level0 row5\" >et</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col0\" class=\"data row5 col0\" >Extra Trees Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col1\" class=\"data row5 col1\" >1033751.0817</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col2\" class=\"data row5 col2\" >4331332918972.5342</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col3\" class=\"data row5 col3\" >2048068.4764</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col4\" class=\"data row5 col4\" >0.4213</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col5\" class=\"data row5 col5\" >1.5354</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col6\" class=\"data row5 col6\" >10.5809</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row5_col7\" class=\"data row5 col7\" >0.1260</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row6\" class=\"row_heading level0 row6\" >catboost</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col0\" class=\"data row6 col0\" >CatBoost Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col1\" class=\"data row6 col1\" >1091839.5605</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col2\" class=\"data row6 col2\" >4388761199683.4858</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col3\" class=\"data row6 col3\" >2077939.4173</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col4\" class=\"data row6 col4\" >0.3959</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col5\" class=\"data row6 col5\" >2.2131</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col6\" class=\"data row6 col6\" >35.4924</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row6_col7\" class=\"data row6 col7\" >1.0180</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row7\" class=\"row_heading level0 row7\" >lightgbm</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col0\" class=\"data row7 col0\" >Light Gradient Boosting Machine</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col1\" class=\"data row7 col1\" >1224993.8391</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col2\" class=\"data row7 col2\" >4495803681110.3721</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col3\" class=\"data row7 col3\" >2095489.2067</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col4\" class=\"data row7 col4\" >0.3946</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col5\" class=\"data row7 col5\" >2.4879</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col6\" class=\"data row7 col6\" >38.6222</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row7_col7\" class=\"data row7 col7\" >0.2200</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row8\" class=\"row_heading level0 row8\" >xgboost</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col0\" class=\"data row8 col0\" >Extreme Gradient Boosting</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col1\" class=\"data row8 col1\" >1137766.3625</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col2\" class=\"data row8 col2\" >4736575288115.2002</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col3\" class=\"data row8 col3\" >2167323.5750</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col4\" class=\"data row8 col4\" >0.3413</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col5\" class=\"data row8 col5\" >1.8721</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col6\" class=\"data row8 col6\" >23.3805</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row8_col7\" class=\"data row8 col7\" >0.1040</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row9\" class=\"row_heading level0 row9\" >huber</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col0\" class=\"data row9 col0\" >Huber Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col1\" class=\"data row9 col1\" >1070230.7345</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col2\" class=\"data row9 col2\" >4803973691466.9619</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col3\" class=\"data row9 col3\" >2162503.2662</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col4\" class=\"data row9 col4\" >0.3672</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col5\" class=\"data row9 col5\" >2.4120</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col6\" class=\"data row9 col6\" >54.0540</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row9_col7\" class=\"data row9 col7\" >0.0300</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row10\" class=\"row_heading level0 row10\" >ada</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col0\" class=\"data row10 col0\" >AdaBoost Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col1\" class=\"data row10 col1\" >1743207.1674</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col2\" class=\"data row10 col2\" >5144363585603.9170</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col3\" class=\"data row10 col3\" >2260806.5900</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col4\" class=\"data row10 col4\" >0.2928</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col5\" class=\"data row10 col5\" >3.7792</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col6\" class=\"data row10 col6\" >399.5518</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row10_col7\" class=\"data row10 col7\" >0.0440</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row11\" class=\"row_heading level0 row11\" >lr</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col0\" class=\"data row11 col0\" >Linear Regression</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col1\" class=\"data row11 col1\" >1379223.0500</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col2\" class=\"data row11 col2\" >5167804055552.0000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col3\" class=\"data row11 col3\" >2268755.4500</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col4\" class=\"data row11 col4\" >0.2766</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col5\" class=\"data row11 col5\" >3.0364</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col6\" class=\"data row11 col6\" >176.5260</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row11_col7\" class=\"data row11 col7\" >1.1720</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row12\" class=\"row_heading level0 row12\" >omp</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col0\" class=\"data row12 col0\" >Orthogonal Matching Pursuit</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col1\" class=\"data row12 col1\" >1206133.4257</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col2\" class=\"data row12 col2\" >5286169007707.4590</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col3\" class=\"data row12 col3\" >2260383.6586</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col4\" class=\"data row12 col4\" >0.2672</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col5\" class=\"data row12 col5\" >2.5499</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col6\" class=\"data row12 col6\" >88.5197</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row12_col7\" class=\"data row12 col7\" >0.0080</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row13\" class=\"row_heading level0 row13\" >lasso</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col0\" class=\"data row13 col0\" >Lasso Regression</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col1\" class=\"data row13 col1\" >1331769.5250</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col2\" class=\"data row13 col2\" >5735291971174.4004</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col3\" class=\"data row13 col3\" >2379944.2250</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col4\" class=\"data row13 col4\" >0.1871</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col5\" class=\"data row13 col5\" >2.9146</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col6\" class=\"data row13 col6\" >177.7356</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row13_col7\" class=\"data row13 col7\" >0.0220</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row14\" class=\"row_heading level0 row14\" >knn</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col0\" class=\"data row14 col0\" >K Neighbors Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col1\" class=\"data row14 col1\" >1251796.4375</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col2\" class=\"data row14 col2\" >5908596837580.7998</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col3\" class=\"data row14 col3\" >2387795.4250</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col4\" class=\"data row14 col4\" >0.2283</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col5\" class=\"data row14 col5\" >1.8132</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col6\" class=\"data row14 col6\" >24.5819</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row14_col7\" class=\"data row14 col7\" >0.0160</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row15\" class=\"row_heading level0 row15\" >llar</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col0\" class=\"data row15 col0\" >Lasso Least Angle Regression</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col1\" class=\"data row15 col1\" >1345177.6254</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col2\" class=\"data row15 col2\" >5949599780914.5176</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col3\" class=\"data row15 col3\" >2419735.4839</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col4\" class=\"data row15 col4\" >0.1538</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col5\" class=\"data row15 col5\" >2.9331</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col6\" class=\"data row15 col6\" >179.0336</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row15_col7\" class=\"data row15 col7\" >0.0160</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row16\" class=\"row_heading level0 row16\" >dt</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col0\" class=\"data row16 col0\" >Decision Tree Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col1\" class=\"data row16 col1\" >1211074.8782</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col2\" class=\"data row16 col2\" >6212485749654.9521</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col3\" class=\"data row16 col3\" >2480129.6696</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col4\" class=\"data row16 col4\" >0.1472</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col5\" class=\"data row16 col5\" >1.8766</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col6\" class=\"data row16 col6\" >10.5540</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row16_col7\" class=\"data row16 col7\" >0.0100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row17\" class=\"row_heading level0 row17\" >dummy</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col0\" class=\"data row17 col0\" >Dummy Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col1\" class=\"data row17 col1\" >1760745.3250</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col2\" class=\"data row17 col2\" >7490637922304.0000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col3\" class=\"data row17 col3\" >2718099.3500</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col4\" class=\"data row17 col4\" >-0.0080</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col5\" class=\"data row17 col5\" >3.8344</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col6\" class=\"data row17 col6\" >473.5532</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row17_col7\" class=\"data row17 col7\" >0.0060</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row18\" class=\"row_heading level0 row18\" >par</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col0\" class=\"data row18 col0\" >Passive Aggressive Regressor</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col1\" class=\"data row18 col1\" >2070661.0046</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col2\" class=\"data row18 col2\" >19967742871650.2578</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col3\" class=\"data row18 col3\" >4325896.1570</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col4\" class=\"data row18 col4\" >-1.8871</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col5\" class=\"data row18 col5\" >2.5663</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col6\" class=\"data row18 col6\" >36.1702</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row18_col7\" class=\"data row18 col7\" >0.0100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_cd02590d_d280_11ec_967d_7085c2294e08level0_row19\" class=\"row_heading level0 row19\" >lar</th>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col0\" class=\"data row19 col0\" >Least Angle Regression</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col1\" class=\"data row19 col1\" >805749704185780.0000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col2\" class=\"data row19 col2\" >12006693421487372323397048467456.0000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col3\" class=\"data row19 col3\" >1657181683730842.5000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col4\" class=\"data row19 col4\" >-1924813196842150656.0000</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col5\" class=\"data row19 col5\" >12.3775</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col6\" class=\"data row19 col6\" >20214263612.9542</td>\n",
       "                        <td id=\"T_cd02590d_d280_11ec_967d_7085c2294e08row19_col7\" class=\"data row19 col7\" >0.0180</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15a7b5d0df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10 = compare_models(sort='MSE', n_select=10, fold=5) \n",
    "# 모델들을 비교하여 가장 좋은 순으로 보여줌\n",
    "# sort를 accuracy보다 F1으로 하는것이 더 좋은 f1score랑 accuracy를 낳음,fold 10번하는게 정확도가 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col0,#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col1,#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col2,#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col3,#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col4,#T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col5{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >MAE</th>        <th class=\"col_heading level0 col1\" >MSE</th>        <th class=\"col_heading level0 col2\" >RMSE</th>        <th class=\"col_heading level0 col3\" >R2</th>        <th class=\"col_heading level0 col4\" >RMSLE</th>        <th class=\"col_heading level0 col5\" >MAPE</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col0\" class=\"data row0 col0\" >1042981.4233</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col1\" class=\"data row0 col1\" >3243314757101.7168</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col2\" class=\"data row0 col2\" >1800920.5305</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col3\" class=\"data row0 col3\" >0.4284</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col4\" class=\"data row0 col4\" >3.1528</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row0_col5\" class=\"data row0 col5\" >134.8512</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col0\" class=\"data row1 col0\" >1246989.2849</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col1\" class=\"data row1 col1\" >6051297662771.0762</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col2\" class=\"data row1 col2\" >2459938.5486</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col3\" class=\"data row1 col3\" >0.3653</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col4\" class=\"data row1 col4\" >2.4276</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row1_col5\" class=\"data row1 col5\" >48.0542</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col0\" class=\"data row2 col0\" >1280371.8127</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col1\" class=\"data row2 col1\" >3687975400157.4897</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col2\" class=\"data row2 col2\" >1920410.2166</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col3\" class=\"data row2 col3\" >0.3860</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col4\" class=\"data row2 col4\" >2.7943</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row2_col5\" class=\"data row2 col5\" >90.7361</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col0\" class=\"data row3 col0\" >1169077.7906</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col1\" class=\"data row3 col1\" >2847938183095.9976</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col2\" class=\"data row3 col2\" >1687583.5337</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col3\" class=\"data row3 col3\" >0.5436</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col4\" class=\"data row3 col4\" >2.7329</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row3_col5\" class=\"data row3 col5\" >102.6595</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col0\" class=\"data row4 col0\" >1322759.9543</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col1\" class=\"data row4 col1\" >5711918615627.0303</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col2\" class=\"data row4 col2\" >2389962.0532</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col3\" class=\"data row4 col3\" >0.4142</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col4\" class=\"data row4 col4\" >2.8733</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row4_col5\" class=\"data row4 col5\" >94.5302</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row5\" class=\"row_heading level0 row5\" >Mean</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col0\" class=\"data row5 col0\" >1212436.0531</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col1\" class=\"data row5 col1\" >4308488923750.6626</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col2\" class=\"data row5 col2\" >2051762.9765</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col3\" class=\"data row5 col3\" >0.4275</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col4\" class=\"data row5 col4\" >2.7962</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row5_col5\" class=\"data row5 col5\" >94.1662</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08level0_row6\" class=\"row_heading level0 row6\" >SD</th>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col0\" class=\"data row6 col0\" >98565.0718</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col1\" class=\"data row6 col1\" >1316042454112.4272</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col2\" class=\"data row6 col2\" >314257.2386</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col3\" class=\"data row6 col3\" >0.0620</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col4\" class=\"data row6 col4\" >0.2336</td>\n",
       "                        <td id=\"T_d8a842f5_d280_11ec_bebf_7085c2294e08row6_col5\" class=\"data row6 col5\" >27.8051</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15a7d1c8e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuned_top5 = [tune_model(i, fold=5) for i in top10[:5]]\n",
    "# 상위 4개, 5개로 했을 때 제일 좋은 결과가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col0,#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col1,#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col2,#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col3,#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col4,#T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col5{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >MAE</th>        <th class=\"col_heading level0 col1\" >MSE</th>        <th class=\"col_heading level0 col2\" >RMSE</th>        <th class=\"col_heading level0 col3\" >R2</th>        <th class=\"col_heading level0 col4\" >RMSLE</th>        <th class=\"col_heading level0 col5\" >MAPE</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col0\" class=\"data row0 col0\" >902725.5513</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col1\" class=\"data row0 col1\" >3105301883120.2051</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col2\" class=\"data row0 col2\" >1762186.6766</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col3\" class=\"data row0 col3\" >0.5468</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col4\" class=\"data row0 col4\" >2.9945</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row0_col5\" class=\"data row0 col5\" >108.6820</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col0\" class=\"data row1 col0\" >857912.1702</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col1\" class=\"data row1 col1\" >2027133371388.0437</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col2\" class=\"data row1 col2\" >1423774.3401</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col3\" class=\"data row1 col3\" >0.5340</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col4\" class=\"data row1 col4\" >2.9369</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row1_col5\" class=\"data row1 col5\" >110.3349</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col0\" class=\"data row2 col0\" >1427364.5372</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col1\" class=\"data row2 col1\" >7471688395564.9756</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col2\" class=\"data row2 col2\" >2733438.9321</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col3\" class=\"data row2 col3\" >0.3964</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col4\" class=\"data row2 col4\" >2.3146</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row2_col5\" class=\"data row2 col5\" >35.4312</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col0\" class=\"data row3 col0\" >906692.0843</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col1\" class=\"data row3 col1\" >4234406081168.2974</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col2\" class=\"data row3 col2\" >2057767.2563</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col3\" class=\"data row3 col3\" >0.3517</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col4\" class=\"data row3 col4\" >1.9459</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row3_col5\" class=\"data row3 col5\" >28.8599</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col0\" class=\"data row4 col0\" >1298013.8242</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col1\" class=\"data row4 col1\" >3563592583029.1216</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col2\" class=\"data row4 col2\" >1887748.0189</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col3\" class=\"data row4 col3\" >0.0561</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col4\" class=\"data row4 col4\" >2.6839</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row4_col5\" class=\"data row4 col5\" >37.7465</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col0\" class=\"data row5 col0\" >1126500.7199</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col1\" class=\"data row5 col1\" >3750484288760.0688</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col2\" class=\"data row5 col2\" >1936616.7119</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col3\" class=\"data row5 col3\" >0.5395</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col4\" class=\"data row5 col4\" >2.1663</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row5_col5\" class=\"data row5 col5\" >28.6519</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col0\" class=\"data row6 col0\" >1200414.5720</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col1\" class=\"data row6 col1\" >3270460442686.2231</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col2\" class=\"data row6 col2\" >1808441.4402</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col3\" class=\"data row6 col3\" >0.4343</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col4\" class=\"data row6 col4\" >2.9854</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row6_col5\" class=\"data row6 col5\" >127.5270</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col0\" class=\"data row7 col0\" >1170258.6455</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col1\" class=\"data row7 col1\" >2804157698189.4487</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col2\" class=\"data row7 col2\" >1674561.9422</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col3\" class=\"data row7 col3\" >0.5795</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col4\" class=\"data row7 col4\" >2.4454</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row7_col5\" class=\"data row7 col5\" >65.7985</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col0\" class=\"data row8 col0\" >806061.4059</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col1\" class=\"data row8 col1\" >2653789879857.4292</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col2\" class=\"data row8 col2\" >1629045.6961</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col3\" class=\"data row8 col3\" >0.6770</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col4\" class=\"data row8 col4\" >3.3918</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row8_col5\" class=\"data row8 col5\" >134.2762</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col0\" class=\"data row9 col0\" >1652453.5633</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col1\" class=\"data row9 col1\" >8269760545037.5859</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col2\" class=\"data row9 col2\" >2875719.1353</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col3\" class=\"data row9 col3\" >0.2564</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col4\" class=\"data row9 col4\" >2.2313</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row9_col5\" class=\"data row9 col5\" >21.8226</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col0\" class=\"data row10 col0\" >1134839.7074</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col1\" class=\"data row10 col1\" >4115077516880.1401</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col2\" class=\"data row10 col2\" >1978930.0150</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col3\" class=\"data row10 col3\" >0.4372</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col4\" class=\"data row10 col4\" >2.6096</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row10_col5\" class=\"data row10 col5\" >69.9131</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col0\" class=\"data row11 col0\" >260087.8941</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col1\" class=\"data row11 col1\" >1972965755715.9221</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col2\" class=\"data row11 col2\" >445997.2116</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col3\" class=\"data row11 col3\" >0.1722</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col4\" class=\"data row11 col4\" >0.4368</td>\n",
       "                        <td id=\"T_dbebf5d2_d280_11ec_a261_7085c2294e08row11_col5\" class=\"data row11 col5\" >43.0582</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15a7bd49ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blender_top5 = blend_models(estimator_list=tuned_top5) #보팅(voting)해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model = finalize_model(blender_top5)\n",
    "prediction = predict_model(final_model, data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['Target'] = prediction['Target'].astype(int)\n",
    "prediction['Label'] = prediction['Label'].astype(float)\n",
    "prediction['Label'] = prediction['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085797.8923"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycaret.utils import check_metric\n",
    "check_metric(prediction['Target'], prediction['Label'], metric = 'RMSE') # 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4350552847704.8447"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1score\n",
    "check_metric(prediction['Target'], prediction['Label'], metric = 'MSE') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold 및 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from ngboost import NGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "\n",
    "lgbm = LGBMRegressor(random_state = 518)\n",
    "acc_list = []\n",
    "lgbm_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    lgbm.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = lgbm.predict(val_x)\n",
    "    sub_pred = lgbm.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(val_y, pred))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    lgbm_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression acc 2116782.6724485313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('logistic regression acc',np.sqrt(mean_squared_error(lgbm_pred, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "\n",
    "gbm = GradientBoostingRegressor(random_state = 42)\n",
    "acc_list = []\n",
    "gbm_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    gbm.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = gbm.predict(val_x)\n",
    "    sub_pred = gbm.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(pred, val_y))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    gbm_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm acc 2237619.653127805\n"
     ]
    }
   ],
   "source": [
    "print('gbm acc',np.sqrt(mean_squared_error(gbm_pred, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=16.1278 val_loss=0.0000 scale=1.0000 norm=1564488.2358\n",
      "[iter 100] loss=15.1138 val_loss=0.0000 scale=2.0000 norm=1268465.2202\n",
      "[iter 200] loss=14.5442 val_loss=0.0000 scale=2.0000 norm=925846.8043\n",
      "[iter 300] loss=14.1659 val_loss=0.0000 scale=1.0000 norm=390652.9993\n",
      "[iter 400] loss=13.9441 val_loss=0.0000 scale=1.0000 norm=360362.2545\n",
      "[iter 0] loss=14.1810 val_loss=0.0000 scale=1.0000 norm=402479.4397\n",
      "[iter 100] loss=13.8604 val_loss=0.0000 scale=1.0000 norm=364779.3735\n",
      "[iter 200] loss=13.7816 val_loss=0.0000 scale=0.2500 norm=86607.0855\n",
      "[iter 300] loss=13.7603 val_loss=0.0000 scale=0.1250 norm=42485.5323\n",
      "[iter 400] loss=13.7483 val_loss=0.0000 scale=0.2500 norm=83801.5446\n",
      "[iter 0] loss=14.2114 val_loss=0.0000 scale=0.5000 norm=171291.6233\n",
      "[iter 100] loss=13.7880 val_loss=0.0000 scale=0.5000 norm=159570.6135\n",
      "[iter 200] loss=13.7694 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 300] loss=13.7694 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 400] loss=13.7694 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 0] loss=13.7702 val_loss=0.0000 scale=1.0000 norm=326372.3104\n",
      "[iter 100] loss=13.6958 val_loss=0.0000 scale=1.0000 norm=306560.2578\n",
      "[iter 200] loss=13.6391 val_loss=0.0000 scale=0.2500 norm=73683.1278\n",
      "[iter 300] loss=13.6042 val_loss=0.0000 scale=0.2500 norm=71763.4151\n",
      "[iter 400] loss=13.5974 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 0] loss=13.6222 val_loss=0.0000 scale=1.0000 norm=292127.9724\n",
      "[iter 100] loss=13.5634 val_loss=0.0000 scale=1.0000 norm=277509.8915\n",
      "[iter 200] loss=13.5319 val_loss=0.0000 scale=0.0625 norm=16821.2108\n",
      "[iter 300] loss=13.5256 val_loss=0.0000 scale=0.0312 norm=8342.8082\n",
      "[iter 400] loss=13.5246 val_loss=0.0000 scale=0.0039 norm=1041.1272\n",
      "[iter 0] loss=13.7813 val_loss=0.0000 scale=1.0000 norm=274115.9303\n",
      "[iter 100] loss=13.4884 val_loss=0.0000 scale=0.5000 norm=126779.4664\n",
      "[iter 200] loss=13.4334 val_loss=0.0000 scale=0.5000 norm=121691.8614\n",
      "[iter 300] loss=13.3994 val_loss=0.0000 scale=0.2500 norm=59197.3584\n",
      "[iter 400] loss=13.3840 val_loss=0.0000 scale=0.0625 norm=14575.8531\n",
      "[iter 0] loss=20.2475 val_loss=0.0000 scale=1.0000 norm=260391.4651\n",
      "[iter 100] loss=13.5839 val_loss=0.0000 scale=1.0000 norm=221724.5364\n",
      "[iter 200] loss=13.4355 val_loss=0.0000 scale=2.0000 norm=414002.3583\n",
      "[iter 300] loss=13.3649 val_loss=0.0000 scale=0.0039 norm=780.8313\n",
      "[iter 400] loss=13.3265 val_loss=0.0000 scale=0.2500 norm=48846.6773\n",
      "[iter 0] loss=13.6054 val_loss=0.0000 scale=1.0000 norm=224232.8246\n",
      "[iter 100] loss=13.3652 val_loss=0.0000 scale=1.0000 norm=203507.3934\n",
      "[iter 200] loss=13.2825 val_loss=0.0000 scale=0.2500 norm=48665.8758\n",
      "[iter 300] loss=13.2478 val_loss=0.0000 scale=0.5000 norm=95159.5342\n",
      "[iter 400] loss=13.2333 val_loss=0.0000 scale=0.1250 norm=23490.1718\n",
      "[iter 0] loss=13.2803 val_loss=0.0000 scale=0.5000 norm=93796.7345\n",
      "[iter 100] loss=13.2439 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 200] loss=13.2439 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 300] loss=13.2439 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 400] loss=13.2439 val_loss=0.0000 scale=0.0000 norm=0.0003\n",
      "[iter 0] loss=13.5843 val_loss=0.0000 scale=1.0000 norm=190430.9454\n",
      "[iter 100] loss=13.2410 val_loss=0.0000 scale=0.1250 norm=22505.9557\n",
      "[iter 200] loss=13.1954 val_loss=0.0000 scale=0.0625 norm=10935.4630\n",
      "[iter 300] loss=13.1537 val_loss=0.0000 scale=0.5000 norm=84866.6015\n",
      "[iter 400] loss=13.1131 val_loss=0.0000 scale=0.5000 norm=81872.1932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "\n",
    "ngb = NGBRegressor(random_state = 518)\n",
    "\n",
    "acc_list = []\n",
    "ngb_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    ngb.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = ngb.predict(val_x)\n",
    "    sub_pred = ngb.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(pred, val_y))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    ngb_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngb acc 2386251.5403697807\n"
     ]
    }
   ],
   "source": [
    "print('ngb acc',np.sqrt(mean_squared_error(ngb_pred, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "rf = RandomForestRegressor(random_state = 518)\n",
    "\n",
    "acc_list = []\n",
    "rf_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    rf.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = ngb.predict(val_x)\n",
    "    sub_pred = ngb.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(pred, val_y))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    rf_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf acc 2400997.7461414416\n"
     ]
    }
   ],
   "source": [
    "print('rf acc',np.sqrt(mean_squared_error(rf_pred, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "\n",
    "cat = CatBoostRegressor(random_state = 518, silent = True)\n",
    "\n",
    "acc_list = []\n",
    "cat_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    cat.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = cat.predict(val_x)\n",
    "    sub_pred = cat.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(pred, val_y))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    cat_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat acc 2205251.5155068124\n"
     ]
    }
   ],
   "source": [
    "print('cat acc',np.sqrt(mean_squared_error(cat_pred, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits =  10, shuffle = True, random_state = 100) # fold 10번하기\n",
    "\n",
    "xgb = XGBRegressor(random_state = 518)\n",
    "\n",
    "acc_list = []\n",
    "xgb_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    xgb.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = xgb.predict(val_x)\n",
    "    sub_pred = xgb.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(pred, val_y))\n",
    "    \n",
    "    acc_list.append(RMSE)\n",
    "    \n",
    "    xgb_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat acc 2294580.819208903\n"
     ]
    }
   ],
   "source": [
    "print('cat acc',np.sqrt(mean_squared_error(xgb_pred, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보팅 (Voting): 투표를 통해 결과 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_vote = (lgbm_pred+ngb_pred+nn_pred)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 2115756.13647301\n"
     ]
    }
   ],
   "source": [
    "print('acc',np.sqrt(mean_squared_error(kfold_vote, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보팅 알고리즘 또한 같은 결과를 보여주지만 안쓰는 이유는 에러가 자주나기 때문에 안정적이지 않음\n",
    "## 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo_clf = VotingClassifier(estimators=[('LR',lr),('LD',lda),('Py',final_model)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting 분류기 정확도 0.7687861271676301\n",
      "f1 0.411764705882353\n"
     ]
    }
   ],
   "source": [
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print(\"Voting 분류기 정확도\", accuracy_score(y_test, pred))\n",
    "print('f1',f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "f1_list = []\n",
    "vo_pred = np.zeros((y_test.shape[0]))\n",
    "for tr_idx, val_idx in kf.split(X_train, y_train) :\n",
    "    tr_x, tr_y = X_train.iloc[tr_idx], y_train.iloc[tr_idx]\n",
    "    val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    vo_clf.fit(tr_x, tr_y)\n",
    "    \n",
    "    pred = vo_clf.predict(val_x)\n",
    "    sub_pred = vo_clf.predict(X_test)\n",
    "    acc = accuracy_score(val_y, pred)\n",
    "    f1_list = f1_score(np.round(vo_pred), y_test)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "    vo_pred += (sub_pred / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985365853658537"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803468208092486"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.round(vo_pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42424242424242425"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(np.round(vo_pred), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_models = [\n",
    "    ('LR',lr),('LD',lda),('RC',rc)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_reg = StackingClassifier(stack_models, final_estimator=xgb, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7572254335260116"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_reg.fit(X_train, y_train)\n",
    "stack_pred = stack_reg.predict(X_test)\n",
    "accuracy_score(stack_pred , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-3df1d4b471bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0macc_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Stacking Ensemble'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-165-775237c5bad4>\u001b[0m in \u001b[0;36macc_eval\u001b[1;34m(name_, actual, pred)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmy_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0my_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "acc_eval('Stacking Ensemble', y_test, stack_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_predictions(name_, actual, pred):\n",
    "    df = pd.DataFrame({'actual': y_test, 'prediction': pred})\n",
    "    df = df.sort_values(by='actual').reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.scatter(df.index, df['prediction'], marker='x', color='r')\n",
    "    plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='black')\n",
    "    plt.title(name_, fontsize=15)\n",
    "    plt.legend(['prediction', 'actual'], fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_eval(name_, actual, pred):\n",
    "    global predictions\n",
    "    global colors\n",
    "\n",
    "    plot_predictions(name_, actual, pred)\n",
    "\n",
    "    mse = accuracy_score(actual, pred)\n",
    "    my_predictions[name_] = mse\n",
    "\n",
    "    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    df = pd.DataFrame(y_value, columns=['model', 'acc'])\n",
    "    print(df)\n",
    "    min_ = df['acc'].min() - 10\n",
    "    max_ = df['acc'].max() + 10\n",
    "    \n",
    "    length = len(df)\n",
    "    \n",
    "    plt.figure(figsize=(10, length))\n",
    "    ax = plt.subplot()\n",
    "    ax.set_yticks(np.arange(len(df)))\n",
    "    ax.set_yticklabels(df['model'], fontsize=15)\n",
    "    bars = ax.barh(np.arange(len(df)), df['acc'])\n",
    "    \n",
    "    for i, v in enumerate(df['acc']):\n",
    "        idx = np.random.choice(len(colors))\n",
    "        bars[i].set_color(colors[idx])\n",
    "        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')\n",
    "        \n",
    "    plt.title('ACC Error', fontsize=18)\n",
    "    plt.xlim(min_, max_)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
